{
  "id": 2487253010,
  "repo_owner": "elastic",
  "repo_name": "elasticsearch",
  "number": 127488,
  "title": "Enable time-series block hash",
  "created_at": "2025-04-28T23:37:19",
  "updated_at": "2025-04-29T00:07:21",
  "state": "open",
  "user_login": "dnhatn",
  "diffs": "diff --git a/x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/aggregation/blockhash/TimeSeriesBlockHash.java b/x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/aggregation/blockhash/TimeSeriesBlockHash.java\nindex 573bb4d723aab..76ad646ef1569 100644\n--- a/x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/aggregation/blockhash/TimeSeriesBlockHash.java\n+++ b/x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/aggregation/blockhash/TimeSeriesBlockHash.java\n@@ -19,6 +19,7 @@\n import org.elasticsearch.compute.data.BlockFactory;\n import org.elasticsearch.compute.data.BytesRefBlock;\n import org.elasticsearch.compute.data.BytesRefVector;\n+import org.elasticsearch.compute.data.ElementType;\n import org.elasticsearch.compute.data.IntBlock;\n import org.elasticsearch.compute.data.IntVector;\n import org.elasticsearch.compute.data.LongBlock;\n@@ -26,11 +27,12 @@\n import org.elasticsearch.compute.data.OrdinalBytesRefBlock;\n import org.elasticsearch.compute.data.OrdinalBytesRefVector;\n import org.elasticsearch.compute.data.Page;\n+import org.elasticsearch.core.Assertions;\n import org.elasticsearch.core.Releasable;\n import org.elasticsearch.core.ReleasableIterator;\n import org.elasticsearch.core.Releasables;\n \n-import java.util.Objects;\n+import java.util.List;\n \n /**\n  * An optimized block hash that receives two blocks: tsid and timestamp, which are sorted.\n@@ -41,7 +43,7 @@ public final class TimeSeriesBlockHash extends BlockHash {\n     private final int tsHashChannel;\n     private final int timestampIntervalChannel;\n \n-    private final BytesRef lastTsid = new BytesRef();\n+    private int lastTsidPosition = 0;\n     private final BytesRefArrayWithSize tsidArray;\n \n     private long lastTimestamp;\n@@ -50,6 +52,8 @@ public final class TimeSeriesBlockHash extends BlockHash {\n     private int currentTimestampCount;\n     private final IntArrayWithSize perTsidCountArray;\n \n+    private final BlockHash assertingHash;\n+\n     public TimeSeriesBlockHash(int tsHashChannel, int timestampIntervalChannel, BlockFactory blockFactory) {\n         super(blockFactory);\n         this.tsHashChannel = tsHashChannel;\n@@ -57,49 +61,111 @@ public TimeSeriesBlockHash(int tsHashChannel, int timestampIntervalChannel, Bloc\n         this.tsidArray = new BytesRefArrayWithSize(blockFactory);\n         this.timestampArray = new LongArrayWithSize(blockFactory);\n         this.perTsidCountArray = new IntArrayWithSize(blockFactory);\n+        if (Assertions.ENABLED) {\n+            final var groupSpecs = List.of(\n+                new GroupSpec(tsHashChannel, ElementType.BYTES_REF),\n+                new GroupSpec(timestampIntervalChannel, ElementType.BYTES_REF)\n+            );\n+            assertingHash = new PackedValuesBlockHash(groupSpecs, blockFactory, Integer.MAX_VALUE);\n+        } else {\n+            assertingHash = null;\n+        }\n     }\n \n     @Override\n     public void close() {\n-        Releasables.close(tsidArray, timestampArray, perTsidCountArray);\n+        Releasables.close(tsidArray, timestampArray, perTsidCountArray, assertingHash);\n+    }\n+\n+    private OrdinalBytesRefVector getTsidVector(Page page) {\n+        BytesRefBlock block = page.getBlock(tsHashChannel);\n+        var ordinalBlock = block.asOrdinals();\n+        if (ordinalBlock == null) {\n+            throw new IllegalStateException(\"expected ordinal block for tsid\");\n+        }\n+        var ordinalVector = ordinalBlock.asVector();\n+        if (ordinalVector == null) {\n+            throw new IllegalStateException(\"expected ordinal vector for tsid\");\n+        }\n+        return ordinalVector;\n+    }\n+\n+    private LongVector getTimestampVector(Page page) {\n+        final LongBlock timestampsBlock = page.getBlock(timestampIntervalChannel);\n+        LongVector timestampsVector = timestampsBlock.asVector();\n+        if (timestampsVector == null) {\n+            throw new IllegalStateException(\"expected long vector for timestamp\");\n+        }\n+        return timestampsVector;\n     }\n \n     @Override\n     public void add(Page page, GroupingAggregatorFunction.AddInput addInput) {\n-        final BytesRefBlock tsidBlock = page.getBlock(tsHashChannel);\n-        final BytesRefVector tsidVector = Objects.requireNonNull(tsidBlock.asVector(), \"tsid input must be a vector\");\n-        final LongBlock timestampBlock = page.getBlock(timestampIntervalChannel);\n-        final LongVector timestampVector = Objects.requireNonNull(timestampBlock.asVector(), \"timestamp input must be a vector\");\n-        try (var ordsBuilder = blockFactory.newIntVectorBuilder(tsidVector.getPositionCount())) {\n+        final BytesRefVector tsidDict;\n+        final IntVector tsidOrdinals;\n+        {\n+            final var tsidVector = getTsidVector(page);\n+            tsidDict = tsidVector.getDictionaryVector();\n+            tsidOrdinals = tsidVector.getOrdinalsVector();\n+        }\n+        try (var ordsBuilder = blockFactory.newIntVectorBuilder(tsidOrdinals.getPositionCount())) {\n             final BytesRef spare = new BytesRef();\n-            // TODO: optimize incoming ordinal block\n-            for (int i = 0; i < tsidVector.getPositionCount(); i++) {\n-                final BytesRef tsid = tsidVector.getBytesRef(i, spare);\n+            final LongVector timestampVector = getTimestampVector(page);\n+            int lastOrd = -1;\n+            for (int i = 0; i < tsidOrdinals.getPositionCount(); i++) {\n+                final int newOrd = tsidOrdinals.getInt(i);\n+                boolean newGroup = false;\n+                if (lastOrd != newOrd) {\n+                    final var newTsid = tsidDict.getBytesRef(newOrd, spare);\n+                    if (positionCount() == 0) {\n+                        newGroup = true;\n+                    } else if (lastOrd == -1) {\n+                        newGroup = lastTsid().equals(newTsid) == false;\n+                    } else {\n+                        newGroup = true;\n+                    }\n+                    if (newGroup) {\n+                        endTsidGroup();\n+                        lastTsidPosition = tsidArray.count;\n+                        tsidArray.append(newTsid);\n+                    }\n+                    lastOrd = newOrd;\n+                }\n                 final long timestamp = timestampVector.getLong(i);\n-                ordsBuilder.appendInt(addOnePosition(tsid, timestamp));\n+                if (newGroup || timestamp != lastTimestamp) {\n+                    assert newGroup || lastTimestamp >= timestamp : \"@timestamp goes backward \" + lastTimestamp + \" < \" + timestamp;\n+                    timestampArray.append(timestamp);\n+                    lastTimestamp = timestamp;\n+                    currentTimestampCount++;\n+                }\n+                ordsBuilder.appendInt(timestampArray.count - 1);\n             }\n             try (var ords = ordsBuilder.build()) {\n                 addInput.add(0, ords);\n+                assert assertingAddInputPage(page, ords);\n             }\n         }\n     }\n \n-    private int addOnePosition(BytesRef tsid, long timestamp) {\n-        boolean newGroup = false;\n-        if (positionCount() == 0 || lastTsid.equals(tsid) == false) {\n-            assert positionCount() == 0 || lastTsid.compareTo(tsid) < 0 : \"tsid goes backward \";\n-            endTsidGroup();\n-            tsidArray.append(tsid);\n-            tsidArray.get(tsidArray.count - 1, lastTsid);\n-            newGroup = true;\n-        }\n-        if (newGroup || timestamp != lastTimestamp) {\n-            assert newGroup || lastTimestamp >= timestamp : \"@timestamp goes backward \" + lastTimestamp + \" < \" + timestamp;\n-            timestampArray.append(timestamp);\n-            lastTimestamp = timestamp;\n-            currentTimestampCount++;\n-        }\n-        return positionCount() - 1;\n+    private boolean assertingAddInputPage(Page page, IntVector actualIds) {\n+        assert assertingHash != null;\n+        assertingHash.add(page, new GroupingAggregatorFunction.AddInput() {\n+            @Override\n+            public void add(int positionOffset, IntBlock groupIds) {\n+                assert false : \"add(IntBlock) shouldn't be called\";\n+            }\n+\n+            @Override\n+            public void add(int positionOffset, IntVector expectedIds) {\n+                assert expectedIds.equals(actualIds) : \"actual=\" + actualIds + \" vs expected = \" + expectedIds;\n+            }\n+\n+            @Override\n+            public void close() {\n+\n+            }\n+        });\n+        return true;\n     }\n \n     private void endTsidGroup() {\n@@ -109,6 +175,12 @@ private void endTsidGroup() {\n         }\n     }\n \n+    private BytesRef lastTsid() {\n+        final BytesRef bytesRef = new BytesRef();\n+        tsidArray.get(lastTsidPosition, bytesRef);\n+        return bytesRef;\n+    }\n+\n     @Override\n     public ReleasableIterator<IntBlock> lookup(Page page, ByteSizeValue targetBlockSize) {\n         throw new UnsupportedOperationException(\"TODO\");\n@@ -125,6 +197,7 @@ public Block[] getKeys() {\n                 blocks[0] = buildTsidBlock();\n             }\n             blocks[1] = timestampArray.toBlock();\n+            assert assertingKeys(blocks);\n             return blocks;\n         } finally {\n             if (blocks[blocks.length - 1] == null) {\n@@ -133,6 +206,18 @@ public Block[] getKeys() {\n         }\n     }\n \n+    private boolean assertingKeys(Block[] actualKeys) {\n+        assert assertingHash != null;\n+        Block[] expectedKeys = assertingHash.getKeys();\n+        try {\n+            assert expectedKeys[0].equals(actualKeys[0]) : \"actual=\" + actualKeys[0] + \" vs expected = \" + expectedKeys[0];\n+            assert expectedKeys[1].equals(actualKeys[1]) : \"actual=\" + actualKeys[1] + \" vs expected = \" + expectedKeys[1];\n+        } finally {\n+            Releasables.close(expectedKeys);\n+        }\n+        return true;\n+    }\n+\n     private BytesRefBlock buildTsidBlockWithOrdinal() {\n         try (IntVector.FixedBuilder ordinalBuilder = blockFactory.newIntVectorFixedBuilder(positionCount())) {\n             for (int i = 0; i < tsidArray.count; i++) {\ndiff --git a/x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/data/OrdinalBytesRefBlock.java b/x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/data/OrdinalBytesRefBlock.java\nindex c4ef3ddf288c3..4d78cbdbb8abe 100644\n--- a/x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/data/OrdinalBytesRefBlock.java\n+++ b/x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/data/OrdinalBytesRefBlock.java\n@@ -75,7 +75,7 @@ public BytesRef getBytesRef(int valueIndex, BytesRef dest) {\n     }\n \n     @Override\n-    public BytesRefVector asVector() {\n+    public OrdinalBytesRefVector asVector() {\n         IntVector vector = ordinals.asVector();\n         if (vector != null) {\n             return new OrdinalBytesRefVector(vector, bytes);\ndiff --git a/x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/operator/TimeSeriesAggregationOperator.java b/x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/operator/TimeSeriesAggregationOperator.java\nindex be79cb48ef867..6ab0291c718a7 100644\n--- a/x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/operator/TimeSeriesAggregationOperator.java\n+++ b/x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/operator/TimeSeriesAggregationOperator.java\n@@ -14,6 +14,7 @@\n import org.elasticsearch.compute.aggregation.GroupingAggregatorEvaluationContext;\n import org.elasticsearch.compute.aggregation.TimeSeriesGroupingAggregatorEvaluationContext;\n import org.elasticsearch.compute.aggregation.blockhash.BlockHash;\n+import org.elasticsearch.compute.aggregation.blockhash.TimeSeriesBlockHash;\n import org.elasticsearch.compute.data.Block;\n import org.elasticsearch.compute.data.ElementType;\n import org.elasticsearch.compute.data.LongBlock;\n@@ -30,6 +31,7 @@ public class TimeSeriesAggregationOperator extends HashAggregationOperator {\n \n     public record Factory(\n         Rounding.Prepared timeBucket,\n+        boolean sortedInput,\n         List<BlockHash.GroupSpec> groups,\n         AggregatorMode aggregatorMode,\n         List<GroupingAggregator.Factory> aggregators,\n@@ -38,17 +40,18 @@ public record Factory(\n         @Override\n         public Operator get(DriverContext driverContext) {\n             // TODO: use TimeSeriesBlockHash when possible\n-            return new TimeSeriesAggregationOperator(\n-                timeBucket,\n-                aggregators,\n-                () -> BlockHash.build(\n-                    groups,\n-                    driverContext.blockFactory(),\n-                    maxPageSize,\n-                    true // we can enable optimizations as the inputs are vectors\n-                ),\n-                driverContext\n-            );\n+            return new TimeSeriesAggregationOperator(timeBucket, aggregators, () -> {\n+                if (sortedInput && groups.size() == 2) {\n+                    return new TimeSeriesBlockHash(groups.get(0).channel(), groups.get(1).channel(), driverContext.blockFactory());\n+                } else {\n+                    return BlockHash.build(\n+                        groups,\n+                        driverContext.blockFactory(),\n+                        maxPageSize,\n+                        true // we can enable optimizations as the inputs are vectors\n+                    );\n+                }\n+            }, driverContext);\n         }\n \n         @Override\ndiff --git a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/AbstractPhysicalOperationProviders.java b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/AbstractPhysicalOperationProviders.java\nindex 92e3a7132b010..bb93865164c49 100644\n--- a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/AbstractPhysicalOperationProviders.java\n+++ b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/AbstractPhysicalOperationProviders.java\n@@ -18,7 +18,6 @@\n import org.elasticsearch.compute.operator.EvalOperator;\n import org.elasticsearch.compute.operator.HashAggregationOperator.HashAggregationOperatorFactory;\n import org.elasticsearch.compute.operator.Operator;\n-import org.elasticsearch.compute.operator.TimeSeriesAggregationOperator;\n import org.elasticsearch.index.analysis.AnalysisRegistry;\n import org.elasticsearch.xpack.esql.EsqlIllegalArgumentException;\n import org.elasticsearch.xpack.esql.core.InvalidArgumentException;\n@@ -175,12 +174,12 @@ else if (aggregatorMode.isOutputPartial()) {\n             );\n             // time-series aggregation\n             if (aggregateExec instanceof TimeSeriesAggregateExec ts) {\n-                operatorFactory = new TimeSeriesAggregationOperator.Factory(\n-                    ts.timeBucketRounding(context.foldCtx()),\n-                    groupSpecs.stream().map(GroupSpec::toHashGroupSpec).toList(),\n+                operatorFactory = timeSeriesAggregatorOperatorFactor(\n+                    ts,\n                     aggregatorMode,\n                     aggregatorFactories,\n-                    context.pageSize(aggregateExec.estimatedRowSize())\n+                    groupSpecs.stream().map(GroupSpec::toHashGroupSpec).toList(),\n+                    context\n                 );\n                 // ordinal grouping\n             } else if (groupSpecs.size() == 1 && groupSpecs.get(0).channel == null) {\n@@ -379,4 +378,12 @@ public abstract Operator.OperatorFactory ordinalGroupingOperatorFactory(\n         ElementType groupType,\n         LocalExecutionPlannerContext context\n     );\n+\n+    public abstract Operator.OperatorFactory timeSeriesAggregatorOperatorFactor(\n+        TimeSeriesAggregateExec ts,\n+        AggregatorMode aggregatorMode,\n+        List<GroupingAggregator.Factory> aggregatorFactories,\n+        List<BlockHash.GroupSpec> groupSpecs,\n+        LocalExecutionPlannerContext context\n+    );\n }\ndiff --git a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/EsPhysicalOperationProviders.java b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/EsPhysicalOperationProviders.java\nindex d60e402153578..899d1bd9a1b4c 100644\n--- a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/EsPhysicalOperationProviders.java\n+++ b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/EsPhysicalOperationProviders.java\n@@ -14,7 +14,9 @@\n import org.apache.lucene.search.IndexSearcher;\n import org.apache.lucene.search.Query;\n import org.elasticsearch.common.logging.HeaderWarning;\n+import org.elasticsearch.compute.aggregation.AggregatorMode;\n import org.elasticsearch.compute.aggregation.GroupingAggregator;\n+import org.elasticsearch.compute.aggregation.blockhash.BlockHash;\n import org.elasticsearch.compute.data.Block;\n import org.elasticsearch.compute.data.ElementType;\n import org.elasticsearch.compute.lucene.DataPartitioning;\n@@ -27,6 +29,7 @@\n import org.elasticsearch.compute.operator.Operator;\n import org.elasticsearch.compute.operator.OrdinalsGroupingOperator;\n import org.elasticsearch.compute.operator.SourceOperator;\n+import org.elasticsearch.compute.operator.TimeSeriesAggregationOperator;\n import org.elasticsearch.core.Nullable;\n import org.elasticsearch.index.IndexMode;\n import org.elasticsearch.index.IndexSettings;\n@@ -61,6 +64,7 @@\n import org.elasticsearch.xpack.esql.plan.physical.EsQueryExec;\n import org.elasticsearch.xpack.esql.plan.physical.EsQueryExec.Sort;\n import org.elasticsearch.xpack.esql.plan.physical.FieldExtractExec;\n+import org.elasticsearch.xpack.esql.plan.physical.TimeSeriesAggregateExec;\n import org.elasticsearch.xpack.esql.plan.physical.TimeSeriesSourceExec;\n import org.elasticsearch.xpack.esql.planner.LocalExecutionPlanner.DriverParallelism;\n import org.elasticsearch.xpack.esql.planner.LocalExecutionPlanner.LocalExecutionPlannerContext;\n@@ -299,6 +303,24 @@ public final Operator.OperatorFactory ordinalGroupingOperatorFactory(\n         );\n     }\n \n+    @Override\n+    public Operator.OperatorFactory timeSeriesAggregatorOperatorFactor(\n+        TimeSeriesAggregateExec ts,\n+        AggregatorMode aggregatorMode,\n+        List<GroupingAggregator.Factory> aggregatorFactories,\n+        List<BlockHash.GroupSpec> groupSpecs,\n+        LocalExecutionPlannerContext context\n+    ) {\n+        return new TimeSeriesAggregationOperator.Factory(\n+            ts.timeBucketRounding(context.foldCtx()),\n+            shardContexts.size() == 1 && ts.anyMatch(p -> p instanceof TimeSeriesSourceExec),\n+            groupSpecs,\n+            aggregatorMode,\n+            aggregatorFactories,\n+            context.pageSize(ts.estimatedRowSize())\n+        );\n+    }\n+\n     public static class DefaultShardContext implements ShardContext {\n         private final int index;\n         private final SearchExecutionContext ctx;\ndiff --git a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/PlannerUtils.java b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/PlannerUtils.java\nindex df2da82e3bee0..bee2c0cef1505 100644\n--- a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/PlannerUtils.java\n+++ b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/PlannerUtils.java\n@@ -157,6 +157,16 @@ public static String[] planOriginalIndices(PhysicalPlan plan) {\n         return indices.toArray(String[]::new);\n     }\n \n+    public static boolean requireTimeSeriesSource(PhysicalPlan plan) {\n+        return plan.anyMatch(p -> {\n+            if (p instanceof FragmentExec f) {\n+                return f.fragment().anyMatch(l -> l instanceof EsRelation s && s.indexMode() == IndexMode.TIME_SERIES);\n+            } else {\n+                return false;\n+            }\n+        });\n+    }\n+\n     private static void forEachRelation(PhysicalPlan plan, Consumer<EsRelation> action) {\n         plan.forEachDown(FragmentExec.class, f -> f.fragment().forEachDown(EsRelation.class, r -> {\n             if (r.indexMode() != IndexMode.LOOKUP) {\ndiff --git a/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/planner/TestPhysicalOperationProviders.java b/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/planner/TestPhysicalOperationProviders.java\nindex ae07d8b1b5975..3fb30fe5a923b 100644\n--- a/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/planner/TestPhysicalOperationProviders.java\n+++ b/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/planner/TestPhysicalOperationProviders.java\n@@ -13,6 +13,7 @@\n import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.common.util.BigArrays;\n import org.elasticsearch.compute.Describable;\n+import org.elasticsearch.compute.aggregation.AggregatorMode;\n import org.elasticsearch.compute.aggregation.GroupingAggregator;\n import org.elasticsearch.compute.aggregation.blockhash.BlockHash;\n import org.elasticsearch.compute.data.Block;\n@@ -31,6 +32,7 @@\n import org.elasticsearch.compute.operator.OrdinalsGroupingOperator;\n import org.elasticsearch.compute.operator.SourceOperator;\n import org.elasticsearch.compute.operator.SourceOperator.SourceOperatorFactory;\n+import org.elasticsearch.compute.operator.TimeSeriesAggregationOperator;\n import org.elasticsearch.compute.test.TestBlockFactory;\n import org.elasticsearch.core.Nullable;\n import org.elasticsearch.env.Environment;\n@@ -58,6 +60,7 @@\n import org.elasticsearch.xpack.esql.plan.physical.AggregateExec;\n import org.elasticsearch.xpack.esql.plan.physical.EsQueryExec;\n import org.elasticsearch.xpack.esql.plan.physical.FieldExtractExec;\n+import org.elasticsearch.xpack.esql.plan.physical.TimeSeriesAggregateExec;\n import org.elasticsearch.xpack.esql.plan.physical.TimeSeriesSourceExec;\n import org.elasticsearch.xpack.esql.planner.LocalExecutionPlanner.LocalExecutionPlannerContext;\n import org.elasticsearch.xpack.esql.planner.LocalExecutionPlanner.PhysicalOperation;\n@@ -153,6 +156,24 @@ public Operator.OperatorFactory ordinalGroupingOperatorFactory(\n         );\n     }\n \n+    @Override\n+    public Operator.OperatorFactory timeSeriesAggregatorOperatorFactor(\n+        TimeSeriesAggregateExec ts,\n+        AggregatorMode aggregatorMode,\n+        List<GroupingAggregator.Factory> aggregatorFactories,\n+        List<BlockHash.GroupSpec> groupSpecs,\n+        LocalExecutionPlannerContext context\n+    ) {\n+        return new TimeSeriesAggregationOperator.Factory(\n+            ts.timeBucketRounding(context.foldCtx()),\n+            false,\n+            groupSpecs,\n+            aggregatorMode,\n+            aggregatorFactories,\n+            context.pageSize(ts.estimatedRowSize())\n+        );\n+    }\n+\n     private class TestSourceOperator extends SourceOperator {\n         private int index = 0;\n         private final DriverContext driverContext;\n",
  "files_changed": 7,
  "additions": 193,
  "deletions": 45,
  "commit_count": 1,
  "mergeable_state": "",
  "base_commit_sha": "9106a44ec5bdd89ef2c57e3d0cf4afc361705988",
  "base_commit_link": "https://github.com/elastic/elasticsearch/commit/9106a44ec5bdd89ef2c57e3d0cf4afc361705988",
  "last_processed_time": "2025-04-29T01:17:35",
  "comments": [],
  "github_reviews": [],
  "patches": [
    {
      "id": 1855,
      "pr_id": 2487253010,
      "path": "x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/aggregation/blockhash/TimeSeriesBlockHash.java",
      "patch": "@@ -19,18 +19,20 @@\n import org.elasticsearch.compute.data.BlockFactory;\n import org.elasticsearch.compute.data.BytesRefBlock;\n import org.elasticsearch.compute.data.BytesRefVector;\n+import org.elasticsearch.compute.data.ElementType;\n import org.elasticsearch.compute.data.IntBlock;\n import org.elasticsearch.compute.data.IntVector;\n import org.elasticsearch.compute.data.LongBlock;\n import org.elasticsearch.compute.data.LongVector;\n import org.elasticsearch.compute.data.OrdinalBytesRefBlock;\n import org.elasticsearch.compute.data.OrdinalBytesRefVector;\n import org.elasticsearch.compute.data.Page;\n+import org.elasticsearch.core.Assertions;\n import org.elasticsearch.core.Releasable;\n import org.elasticsearch.core.ReleasableIterator;\n import org.elasticsearch.core.Releasables;\n \n-import java.util.Objects;\n+import java.util.List;\n \n /**\n  * An optimized block hash that receives two blocks: tsid and timestamp, which are sorted.\n@@ -41,7 +43,7 @@ public final class TimeSeriesBlockHash extends BlockHash {\n     private final int tsHashChannel;\n     private final int timestampIntervalChannel;\n \n-    private final BytesRef lastTsid = new BytesRef();\n+    private int lastTsidPosition = 0;\n     private final BytesRefArrayWithSize tsidArray;\n \n     private long lastTimestamp;\n@@ -50,56 +52,120 @@ public final class TimeSeriesBlockHash extends BlockHash {\n     private int currentTimestampCount;\n     private final IntArrayWithSize perTsidCountArray;\n \n+    private final BlockHash assertingHash;\n+\n     public TimeSeriesBlockHash(int tsHashChannel, int timestampIntervalChannel, BlockFactory blockFactory) {\n         super(blockFactory);\n         this.tsHashChannel = tsHashChannel;\n         this.timestampIntervalChannel = timestampIntervalChannel;\n         this.tsidArray = new BytesRefArrayWithSize(blockFactory);\n         this.timestampArray = new LongArrayWithSize(blockFactory);\n         this.perTsidCountArray = new IntArrayWithSize(blockFactory);\n+        if (Assertions.ENABLED) {\n+            final var groupSpecs = List.of(\n+                new GroupSpec(tsHashChannel, ElementType.BYTES_REF),\n+                new GroupSpec(timestampIntervalChannel, ElementType.BYTES_REF)\n+            );\n+            assertingHash = new PackedValuesBlockHash(groupSpecs, blockFactory, Integer.MAX_VALUE);\n+        } else {\n+            assertingHash = null;\n+        }\n     }\n \n     @Override\n     public void close() {\n-        Releasables.close(tsidArray, timestampArray, perTsidCountArray);\n+        Releasables.close(tsidArray, timestampArray, perTsidCountArray, assertingHash);\n+    }\n+\n+    private OrdinalBytesRefVector getTsidVector(Page page) {\n+        BytesRefBlock block = page.getBlock(tsHashChannel);\n+        var ordinalBlock = block.asOrdinals();\n+        if (ordinalBlock == null) {\n+            throw new IllegalStateException(\"expected ordinal block for tsid\");\n+        }\n+        var ordinalVector = ordinalBlock.asVector();\n+        if (ordinalVector == null) {\n+            throw new IllegalStateException(\"expected ordinal vector for tsid\");\n+        }\n+        return ordinalVector;\n+    }\n+\n+    private LongVector getTimestampVector(Page page) {\n+        final LongBlock timestampsBlock = page.getBlock(timestampIntervalChannel);\n+        LongVector timestampsVector = timestampsBlock.asVector();\n+        if (timestampsVector == null) {\n+            throw new IllegalStateException(\"expected long vector for timestamp\");\n+        }\n+        return timestampsVector;\n     }\n \n     @Override\n     public void add(Page page, GroupingAggregatorFunction.AddInput addInput) {\n-        final BytesRefBlock tsidBlock = page.getBlock(tsHashChannel);\n-        final BytesRefVector tsidVector = Objects.requireNonNull(tsidBlock.asVector(), \"tsid input must be a vector\");\n-        final LongBlock timestampBlock = page.getBlock(timestampIntervalChannel);\n-        final LongVector timestampVector = Objects.requireNonNull(timestampBlock.asVector(), \"timestamp input must be a vector\");\n-        try (var ordsBuilder = blockFactory.newIntVectorBuilder(tsidVector.getPositionCount())) {\n+        final BytesRefVector tsidDict;\n+        final IntVector tsidOrdinals;\n+        {\n+            final var tsidVector = getTsidVector(page);\n+            tsidDict = tsidVector.getDictionaryVector();\n+            tsidOrdinals = tsidVector.getOrdinalsVector();\n+        }\n+        try (var ordsBuilder = blockFactory.newIntVectorBuilder(tsidOrdinals.getPositionCount())) {\n             final BytesRef spare = new BytesRef();\n-            // TODO: optimize incoming ordinal block\n-            for (int i = 0; i < tsidVector.getPositionCount(); i++) {\n-                final BytesRef tsid = tsidVector.getBytesRef(i, spare);\n+            final LongVector timestampVector = getTimestampVector(page);\n+            int lastOrd = -1;\n+            for (int i = 0; i < tsidOrdinals.getPositionCount(); i++) {\n+                final int newOrd = tsidOrdinals.getInt(i);\n+                boolean newGroup = false;\n+                if (lastOrd != newOrd) {\n+                    final var newTsid = tsidDict.getBytesRef(newOrd, spare);\n+                    if (positionCount() == 0) {\n+                        newGroup = true;\n+                    } else if (lastOrd == -1) {\n+                        newGroup = lastTsid().equals(newTsid) == false;\n+                    } else {\n+                        newGroup = true;\n+                    }\n+                    if (newGroup) {\n+                        endTsidGroup();\n+                        lastTsidPosition = tsidArray.count;\n+                        tsidArray.append(newTsid);\n+                    }\n+                    lastOrd = newOrd;\n+                }\n                 final long timestamp = timestampVector.getLong(i);\n-                ordsBuilder.appendInt(addOnePosition(tsid, timestamp));\n+                if (newGroup || timestamp != lastTimestamp) {\n+                    assert newGroup || lastTimestamp >= timestamp : \"@timestamp goes backward \" + lastTimestamp + \" < \" + timestamp;\n+                    timestampArray.append(timestamp);\n+                    lastTimestamp = timestamp;\n+                    currentTimestampCount++;\n+                }\n+                ordsBuilder.appendInt(timestampArray.count - 1);\n             }\n             try (var ords = ordsBuilder.build()) {\n                 addInput.add(0, ords);\n+                assert assertingAddInputPage(page, ords);\n             }\n         }\n     }\n \n-    private int addOnePosition(BytesRef tsid, long timestamp) {\n-        boolean newGroup = false;\n-        if (positionCount() == 0 || lastTsid.equals(tsid) == false) {\n-            assert positionCount() == 0 || lastTsid.compareTo(tsid) < 0 : \"tsid goes backward \";\n-            endTsidGroup();\n-            tsidArray.append(tsid);\n-            tsidArray.get(tsidArray.count - 1, lastTsid);\n-            newGroup = true;\n-        }\n-        if (newGroup || timestamp != lastTimestamp) {\n-            assert newGroup || lastTimestamp >= timestamp : \"@timestamp goes backward \" + lastTimestamp + \" < \" + timestamp;\n-            timestampArray.append(timestamp);\n-            lastTimestamp = timestamp;\n-            currentTimestampCount++;\n-        }\n-        return positionCount() - 1;\n+    private boolean assertingAddInputPage(Page page, IntVector actualIds) {\n+        assert assertingHash != null;\n+        assertingHash.add(page, new GroupingAggregatorFunction.AddInput() {\n+            @Override\n+            public void add(int positionOffset, IntBlock groupIds) {\n+                assert false : \"add(IntBlock) shouldn't be called\";\n+            }\n+\n+            @Override\n+            public void add(int positionOffset, IntVector expectedIds) {\n+                assert expectedIds.equals(actualIds) : \"actual=\" + actualIds + \" vs expected = \" + expectedIds;\n+            }\n+\n+            @Override\n+            public void close() {\n+\n+            }\n+        });\n+        return true;\n     }\n \n     private void endTsidGroup() {\n@@ -109,6 +175,12 @@ private void endTsidGroup() {\n         }\n     }\n \n+    private BytesRef lastTsid() {\n+        final BytesRef bytesRef = new BytesRef();\n+        tsidArray.get(lastTsidPosition, bytesRef);\n+        return bytesRef;\n+    }\n+\n     @Override\n     public ReleasableIterator<IntBlock> lookup(Page page, ByteSizeValue targetBlockSize) {\n         throw new UnsupportedOperationException(\"TODO\");\n@@ -125,6 +197,7 @@ public Block[] getKeys() {\n                 blocks[0] = buildTsidBlock();\n             }\n             blocks[1] = timestampArray.toBlock();\n+            assert assertingKeys(blocks);\n             return blocks;\n         } finally {\n             if (blocks[blocks.length - 1] == null) {\n@@ -133,6 +206,18 @@ public Block[] getKeys() {\n         }\n     }\n \n+    private boolean assertingKeys(Block[] actualKeys) {\n+        assert assertingHash != null;\n+        Block[] expectedKeys = assertingHash.getKeys();\n+        try {\n+            assert expectedKeys[0].equals(actualKeys[0]) : \"actual=\" + actualKeys[0] + \" vs expected = \" + expectedKeys[0];\n+            assert expectedKeys[1].equals(actualKeys[1]) : \"actual=\" + actualKeys[1] + \" vs expected = \" + expectedKeys[1];\n+        } finally {\n+            Releasables.close(expectedKeys);\n+        }\n+        return true;\n+    }\n+\n     private BytesRefBlock buildTsidBlockWithOrdinal() {\n         try (IntVector.FixedBuilder ordinalBuilder = blockFactory.newIntVectorFixedBuilder(positionCount())) {\n             for (int i = 0; i < tsidArray.count; i++) {",
      "filename": "x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/aggregation/blockhash/TimeSeriesBlockHash.java",
      "status": "modified",
      "changes": 141,
      "additions": 113,
      "deletions": 28
    },
    {
      "id": 1857,
      "pr_id": 2487253010,
      "path": "x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/data/OrdinalBytesRefBlock.java",
      "patch": "@@ -75,7 +75,7 @@ public BytesRef getBytesRef(int valueIndex, BytesRef dest) {\n     }\n \n     @Override\n-    public BytesRefVector asVector() {\n+    public OrdinalBytesRefVector asVector() {\n         IntVector vector = ordinals.asVector();\n         if (vector != null) {\n             return new OrdinalBytesRefVector(vector, bytes);",
      "filename": "x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/data/OrdinalBytesRefBlock.java",
      "status": "modified",
      "changes": 2,
      "additions": 1,
      "deletions": 1
    },
    {
      "id": 1859,
      "pr_id": 2487253010,
      "path": "x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/operator/TimeSeriesAggregationOperator.java",
      "patch": "@@ -14,6 +14,7 @@\n import org.elasticsearch.compute.aggregation.GroupingAggregatorEvaluationContext;\n import org.elasticsearch.compute.aggregation.TimeSeriesGroupingAggregatorEvaluationContext;\n import org.elasticsearch.compute.aggregation.blockhash.BlockHash;\n+import org.elasticsearch.compute.aggregation.blockhash.TimeSeriesBlockHash;\n import org.elasticsearch.compute.data.Block;\n import org.elasticsearch.compute.data.ElementType;\n import org.elasticsearch.compute.data.LongBlock;\n@@ -30,6 +31,7 @@ public class TimeSeriesAggregationOperator extends HashAggregationOperator {\n \n     public record Factory(\n         Rounding.Prepared timeBucket,\n+        boolean sortedInput,\n         List<BlockHash.GroupSpec> groups,\n         AggregatorMode aggregatorMode,\n         List<GroupingAggregator.Factory> aggregators,\n@@ -38,17 +40,18 @@ public record Factory(\n         @Override\n         public Operator get(DriverContext driverContext) {\n             // TODO: use TimeSeriesBlockHash when possible\n-            return new TimeSeriesAggregationOperator(\n-                timeBucket,\n-                aggregators,\n-                () -> BlockHash.build(\n-                    groups,\n-                    driverContext.blockFactory(),\n-                    maxPageSize,\n-                    true // we can enable optimizations as the inputs are vectors\n-                ),\n-                driverContext\n-            );\n+            return new TimeSeriesAggregationOperator(timeBucket, aggregators, () -> {\n+                if (sortedInput && groups.size() == 2) {\n+                    return new TimeSeriesBlockHash(groups.get(0).channel(), groups.get(1).channel(), driverContext.blockFactory());\n+                } else {\n+                    return BlockHash.build(\n+                        groups,\n+                        driverContext.blockFactory(),\n+                        maxPageSize,\n+                        true // we can enable optimizations as the inputs are vectors\n+                    );\n+                }\n+            }, driverContext);\n         }\n \n         @Override",
      "filename": "x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/operator/TimeSeriesAggregationOperator.java",
      "status": "modified",
      "changes": 25,
      "additions": 14,
      "deletions": 11
    },
    {
      "id": 1861,
      "pr_id": 2487253010,
      "path": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/AbstractPhysicalOperationProviders.java",
      "patch": "@@ -18,7 +18,6 @@\n import org.elasticsearch.compute.operator.EvalOperator;\n import org.elasticsearch.compute.operator.HashAggregationOperator.HashAggregationOperatorFactory;\n import org.elasticsearch.compute.operator.Operator;\n-import org.elasticsearch.compute.operator.TimeSeriesAggregationOperator;\n import org.elasticsearch.index.analysis.AnalysisRegistry;\n import org.elasticsearch.xpack.esql.EsqlIllegalArgumentException;\n import org.elasticsearch.xpack.esql.core.InvalidArgumentException;\n@@ -175,12 +174,12 @@ else if (aggregatorMode.isOutputPartial()) {\n             );\n             // time-series aggregation\n             if (aggregateExec instanceof TimeSeriesAggregateExec ts) {\n-                operatorFactory = new TimeSeriesAggregationOperator.Factory(\n-                    ts.timeBucketRounding(context.foldCtx()),\n-                    groupSpecs.stream().map(GroupSpec::toHashGroupSpec).toList(),\n+                operatorFactory = timeSeriesAggregatorOperatorFactor(\n+                    ts,\n                     aggregatorMode,\n                     aggregatorFactories,\n-                    context.pageSize(aggregateExec.estimatedRowSize())\n+                    groupSpecs.stream().map(GroupSpec::toHashGroupSpec).toList(),\n+                    context\n                 );\n                 // ordinal grouping\n             } else if (groupSpecs.size() == 1 && groupSpecs.get(0).channel == null) {\n@@ -379,4 +378,12 @@ public abstract Operator.OperatorFactory ordinalGroupingOperatorFactory(\n         ElementType groupType,\n         LocalExecutionPlannerContext context\n     );\n+\n+    public abstract Operator.OperatorFactory timeSeriesAggregatorOperatorFactor(\n+        TimeSeriesAggregateExec ts,\n+        AggregatorMode aggregatorMode,\n+        List<GroupingAggregator.Factory> aggregatorFactories,\n+        List<BlockHash.GroupSpec> groupSpecs,\n+        LocalExecutionPlannerContext context\n+    );\n }",
      "filename": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/AbstractPhysicalOperationProviders.java",
      "status": "modified",
      "changes": 17,
      "additions": 12,
      "deletions": 5
    },
    {
      "id": 1862,
      "pr_id": 2487253010,
      "path": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/EsPhysicalOperationProviders.java",
      "patch": "@@ -14,7 +14,9 @@\n import org.apache.lucene.search.IndexSearcher;\n import org.apache.lucene.search.Query;\n import org.elasticsearch.common.logging.HeaderWarning;\n+import org.elasticsearch.compute.aggregation.AggregatorMode;\n import org.elasticsearch.compute.aggregation.GroupingAggregator;\n+import org.elasticsearch.compute.aggregation.blockhash.BlockHash;\n import org.elasticsearch.compute.data.Block;\n import org.elasticsearch.compute.data.ElementType;\n import org.elasticsearch.compute.lucene.DataPartitioning;\n@@ -27,6 +29,7 @@\n import org.elasticsearch.compute.operator.Operator;\n import org.elasticsearch.compute.operator.OrdinalsGroupingOperator;\n import org.elasticsearch.compute.operator.SourceOperator;\n+import org.elasticsearch.compute.operator.TimeSeriesAggregationOperator;\n import org.elasticsearch.core.Nullable;\n import org.elasticsearch.index.IndexMode;\n import org.elasticsearch.index.IndexSettings;\n@@ -61,6 +64,7 @@\n import org.elasticsearch.xpack.esql.plan.physical.EsQueryExec;\n import org.elasticsearch.xpack.esql.plan.physical.EsQueryExec.Sort;\n import org.elasticsearch.xpack.esql.plan.physical.FieldExtractExec;\n+import org.elasticsearch.xpack.esql.plan.physical.TimeSeriesAggregateExec;\n import org.elasticsearch.xpack.esql.plan.physical.TimeSeriesSourceExec;\n import org.elasticsearch.xpack.esql.planner.LocalExecutionPlanner.DriverParallelism;\n import org.elasticsearch.xpack.esql.planner.LocalExecutionPlanner.LocalExecutionPlannerContext;\n@@ -299,6 +303,24 @@ public final Operator.OperatorFactory ordinalGroupingOperatorFactory(\n         );\n     }\n \n+    @Override\n+    public Operator.OperatorFactory timeSeriesAggregatorOperatorFactor(\n+        TimeSeriesAggregateExec ts,\n+        AggregatorMode aggregatorMode,\n+        List<GroupingAggregator.Factory> aggregatorFactories,\n+        List<BlockHash.GroupSpec> groupSpecs,\n+        LocalExecutionPlannerContext context\n+    ) {\n+        return new TimeSeriesAggregationOperator.Factory(\n+            ts.timeBucketRounding(context.foldCtx()),\n+            shardContexts.size() == 1 && ts.anyMatch(p -> p instanceof TimeSeriesSourceExec),\n+            groupSpecs,\n+            aggregatorMode,\n+            aggregatorFactories,\n+            context.pageSize(ts.estimatedRowSize())\n+        );\n+    }\n+\n     public static class DefaultShardContext implements ShardContext {\n         private final int index;\n         private final SearchExecutionContext ctx;",
      "filename": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/EsPhysicalOperationProviders.java",
      "status": "modified",
      "changes": 22,
      "additions": 22,
      "deletions": 0
    },
    {
      "id": 1864,
      "pr_id": 2487253010,
      "path": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/PlannerUtils.java",
      "patch": "@@ -157,6 +157,16 @@ public static String[] planOriginalIndices(PhysicalPlan plan) {\n         return indices.toArray(String[]::new);\n     }\n \n+    public static boolean requireTimeSeriesSource(PhysicalPlan plan) {\n+        return plan.anyMatch(p -> {\n+            if (p instanceof FragmentExec f) {\n+                return f.fragment().anyMatch(l -> l instanceof EsRelation s && s.indexMode() == IndexMode.TIME_SERIES);\n+            } else {\n+                return false;\n+            }\n+        });\n+    }\n+\n     private static void forEachRelation(PhysicalPlan plan, Consumer<EsRelation> action) {\n         plan.forEachDown(FragmentExec.class, f -> f.fragment().forEachDown(EsRelation.class, r -> {\n             if (r.indexMode() != IndexMode.LOOKUP) {",
      "filename": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/PlannerUtils.java",
      "status": "modified",
      "changes": 10,
      "additions": 10,
      "deletions": 0
    },
    {
      "id": 1865,
      "pr_id": 2487253010,
      "path": "x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/planner/TestPhysicalOperationProviders.java",
      "patch": "@@ -13,6 +13,7 @@\n import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.common.util.BigArrays;\n import org.elasticsearch.compute.Describable;\n+import org.elasticsearch.compute.aggregation.AggregatorMode;\n import org.elasticsearch.compute.aggregation.GroupingAggregator;\n import org.elasticsearch.compute.aggregation.blockhash.BlockHash;\n import org.elasticsearch.compute.data.Block;\n@@ -31,6 +32,7 @@\n import org.elasticsearch.compute.operator.OrdinalsGroupingOperator;\n import org.elasticsearch.compute.operator.SourceOperator;\n import org.elasticsearch.compute.operator.SourceOperator.SourceOperatorFactory;\n+import org.elasticsearch.compute.operator.TimeSeriesAggregationOperator;\n import org.elasticsearch.compute.test.TestBlockFactory;\n import org.elasticsearch.core.Nullable;\n import org.elasticsearch.env.Environment;\n@@ -58,6 +60,7 @@\n import org.elasticsearch.xpack.esql.plan.physical.AggregateExec;\n import org.elasticsearch.xpack.esql.plan.physical.EsQueryExec;\n import org.elasticsearch.xpack.esql.plan.physical.FieldExtractExec;\n+import org.elasticsearch.xpack.esql.plan.physical.TimeSeriesAggregateExec;\n import org.elasticsearch.xpack.esql.plan.physical.TimeSeriesSourceExec;\n import org.elasticsearch.xpack.esql.planner.LocalExecutionPlanner.LocalExecutionPlannerContext;\n import org.elasticsearch.xpack.esql.planner.LocalExecutionPlanner.PhysicalOperation;\n@@ -153,6 +156,24 @@ public Operator.OperatorFactory ordinalGroupingOperatorFactory(\n         );\n     }\n \n+    @Override\n+    public Operator.OperatorFactory timeSeriesAggregatorOperatorFactor(\n+        TimeSeriesAggregateExec ts,\n+        AggregatorMode aggregatorMode,\n+        List<GroupingAggregator.Factory> aggregatorFactories,\n+        List<BlockHash.GroupSpec> groupSpecs,\n+        LocalExecutionPlannerContext context\n+    ) {\n+        return new TimeSeriesAggregationOperator.Factory(\n+            ts.timeBucketRounding(context.foldCtx()),\n+            false,\n+            groupSpecs,\n+            aggregatorMode,\n+            aggregatorFactories,\n+            context.pageSize(ts.estimatedRowSize())\n+        );\n+    }\n+\n     private class TestSourceOperator extends SourceOperator {\n         private int index = 0;\n         private final DriverContext driverContext;",
      "filename": "x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/planner/TestPhysicalOperationProviders.java",
      "status": "modified",
      "changes": 21,
      "additions": 21,
      "deletions": 0
    }
  ],
  "ai_reviews": [
    {
      "id": 9,
      "pr_id": 2487253010,
      "summary": "This pull request (PR) titled \"Enable time-series block hash\" introduces enhancements to the Elasticsearch ESQL plugin, specifically focusing on time-series data processing. The primary purpose of these changes is to implement a new mechanism for calculating block hashes in time-series data, which can improve performance and data integrity during aggregation operations.\n\nSignificant modifications include the addition of 113 lines and deletion of 28 lines in the `TimeSeriesBlockHash.java` file, which likely contains the core logic for the new hashing mechanism. Other files, such as `TimeSeriesAggregationOperator.java` and `AbstractPhysicalOperationProviders.java`, have also been updated to integrate this new functionality, with a total of 14 and 12 lines added respectively. The changes reflect a comprehensive update to support the new time-series block hash feature across various components of the ESQL plugin.\n\nPotential concerns include the need for thorough testing to ensure that the new hashing mechanism does not introduce regressions or performance issues, especially given the complexity of time-series data. Additionally, it would be beneficial to include documentation or comments explaining the rationale behind the changes and how they impact existing functionality. Overall, the PR appears to be a significant step forward in enhancing time-series data handling within the Elasticsearch ecosystem.",
      "full_review": "# AI Review \ud83e\udd16\n\n## Summary\nThis pull request (PR) titled \"Enable time-series block hash\" introduces enhancements to the Elasticsearch ESQL plugin, specifically focusing on time-series data processing. The primary purpose of these changes is to implement a new mechanism for calculating block hashes in time-series data, which can improve performance and data integrity during aggregation operations.\n\nSignificant modifications include the addition of 113 lines and deletion of 28 lines in the `TimeSeriesBlockHash.java` file, which likely contains the core logic for the new hashing mechanism. Other files, such as `TimeSeriesAggregationOperator.java` and `AbstractPhysicalOperationProviders.java`, have also been updated to integrate this new functionality, with a total of 14 and 12 lines added respectively. The changes reflect a comprehensive update to support the new time-series block hash feature across various components of the ESQL plugin.\n\nPotential concerns include the need for thorough testing to ensure that the new hashing mechanism does not introduce regressions or performance issues, especially given the complexity of time-series data. Additionally, it would be beneficial to include documentation or comments explaining the rationale behind the changes and how they impact existing functionality. Overall, the PR appears to be a significant step forward in enhancing time-series data handling within the Elasticsearch ecosystem.\n\n## Detailed Review\n\n### x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/aggregation/blockhash/TimeSeriesBlockHash.java\nOverall, the code diff shows a significant refactor of the `TimeSeriesBlockHash` class, introducing new methods and improving the handling of timestamp and tsid data. However, there are a few areas that could be improved or warrant further attention.\n\n### Specific Issues:\n\n1. **Assertions Handling (Lines 43-45)**:\n   - The use of `Assertions.ENABLED` to conditionally create `assertingHash` is good for performance, but ensure that the assertions are properly configured in the build environment. If assertions are disabled in production, the `assertingHash` will be `null`, which could lead to `NullPointerException` if not handled correctly in the `assertingAddInputPage` method.\n\n2. **Error Handling (Lines 66-68)**:\n   - The `IllegalStateException` messages in `getTsidVector` and `getTimestampVector` methods are clear, but consider logging these exceptions for better traceability in production environments. This can help in debugging issues when the expected data types are not met.\n\n3. **Performance Considerations (Lines 83-84)**:\n   - The loop that processes `tsidOrdinals` could be optimized further. The current implementation checks for `newGroup` and `lastOrd` in every iteration. If the data is large, this could lead to performance bottlenecks. Consider batching or reducing the number of checks if possible.\n\n4. **Code Style and Readability (Lines 83-84)**:\n   - The nested structure of the `if` statements in the `add` method could be simplified for better readability. Consider extracting some of the logic into helper methods to reduce complexity and improve maintainability.\n\n5. **Potential Edge Cases (Lines 83-84)**:\n   - The logic for determining `newGroup` relies heavily on the order of the input data. Ensure that the input data is always sorted as expected. If there are any cases where the data might not be sorted, this could lead to incorrect behavior.\n\n6. **Unused Imports (Line 20)**:\n   - The import statement for `java.util.Objects` is no longer needed since the code now uses `Objects.requireNonNull` only in the commented-out section. Consider removing it to keep the code clean.\n\n7. **Closing Resources (Line 66)**:\n   - In the `close` method, ensure that `assertingHash` is checked for null before attempting to close it. This will prevent potential `NullPointerException` if assertions are disabled.\n\n### Conclusion:\nWhile the changes introduce useful functionality and improve the structure of the code, attention should be paid to error handling, performance optimizations, and ensuring that the code remains maintainable and readable. Addressing the above points will enhance the robustness and clarity of the implementation.\n### x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/data/OrdinalBytesRefBlock.java\nOverall Assessment:\nThe code diff shows a modification in the return type of the `asVector` method from `BytesRefVector` to `OrdinalBytesRefVector`. This change appears to be a refactoring to provide a more specific type, which can improve type safety and clarity. However, it is essential to ensure that this change does not introduce any bugs or logical errors in the surrounding code.\n\nSpecific Issues:\n\n1. **Line 7**: \n   - **Change in Return Type**: Ensure that all calls to `asVector()` in the codebase are updated to handle the new return type `OrdinalBytesRefVector`. If there are any existing references to `BytesRefVector`, they may lead to compilation errors or runtime issues if not updated accordingly.\n\n2. **Type Compatibility**: \n   - Verify that `OrdinalBytesRefVector` is a subclass or compatible with `BytesRefVector`. If it is not, any code that expects a `BytesRefVector` will break. This could lead to runtime exceptions if not handled properly.\n\n3. **Testing**: \n   - Ensure that there are adequate unit tests covering the `asVector()` method. If the method's behavior has changed due to the new return type, additional tests may be necessary to validate the new functionality.\n\n4. **Documentation**: \n   - If there is any documentation or comments that reference `asVector()` returning a `BytesRefVector`, these should be updated to reflect the new return type. This helps maintain clarity for future developers.\n\n5. **Edge Cases**: \n   - Consider any edge cases that may arise from the new vector type. For instance, if `OrdinalBytesRefVector` has different handling for null or empty states compared to `BytesRefVector`, ensure that these cases are tested.\n\nIn summary, while the change appears to be a positive refactor, it is crucial to ensure that all dependent code is updated accordingly, and that thorough testing is conducted to prevent any potential issues.\n### x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/operator/TimeSeriesAggregationOperator.java\nOverall, the code diff appears to be a modification that introduces a new boolean parameter `sortedInput` to the `Factory` record and adjusts the logic in the `get` method accordingly. The changes seem to be aimed at optimizing the aggregation process based on whether the input is sorted. However, there are a few points to consider:\n\n1. **Logical Errors**:\n   - **Line 38-40**: The logic for using `TimeSeriesBlockHash` is contingent on `sortedInput` being true and `groups.size() == 2`. Ensure that this condition is valid for all expected use cases. If `groups.size()` can be less than 2, it may lead to unexpected behavior or exceptions.\n\n2. **Performance Issues**:\n   - **Line 38-40**: The new logic introduces a conditional check for `sortedInput` and `groups.size()`. While this is a good optimization, ensure that the performance gain from using `TimeSeriesBlockHash` outweighs the overhead of the conditional check. It might be beneficial to benchmark both paths to confirm the performance improvement.\n\n3. **Code Style and Best Practices**:\n   - **Line 38-40**: Consider adding comments to clarify why `TimeSeriesBlockHash` is used when `sortedInput` is true and `groups.size() == 2`. This will help future maintainers understand the rationale behind this decision.\n   - **Line 38-40**: The inline lambda for the block hash creation could be extracted into a separate method for better readability and maintainability.\n\n4. **Potential Edge Cases**:\n   - **Line 38-40**: If `groups.size()` is less than 2, the current implementation defaults to using `BlockHash.build`. Ensure that this fallback is appropriate and does not lead to incorrect behavior in scenarios where fewer than two groups are provided.\n   - **Line 38-40**: Consider what happens if `sortedInput` is true but the input data is not actually sorted. There should be validation or documentation to ensure that users of this class understand the requirements for `sortedInput`.\n\nIn summary, while the changes seem to be on the right track, careful consideration of edge cases and performance implications is necessary. Adding comments and possibly refactoring for clarity would also enhance the code quality.\n### x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/AbstractPhysicalOperationProviders.java\nOverall, the code diff appears to be a refactoring of the `AbstractPhysicalOperationProviders` class, specifically focusing on the handling of time-series aggregation. The changes seem to improve the clarity of the code by introducing a new method for creating the time series aggregator operator factory. However, there are a few points worth noting:\n\n1. **Code Clarity**: The new method `timeSeriesAggregatorOperatorFactor` is introduced, which is a good practice for improving code readability. However, the method name has a typo; it should be `timeSeriesAggregatorOperatorFactory` instead of `timeSeriesAggregatorOperatorFactor`. This could lead to confusion.\n\n   - **Line 378**: Rename `timeSeriesAggregatorOperatorFactor` to `timeSeriesAggregatorOperatorFactory`.\n\n2. **Parameter Naming**: The parameter names in the new method could be more descriptive. For example, `ts` could be renamed to `timeSeriesAggregateExec` for better clarity.\n\n   - **Line 378**: Consider renaming `ts` to `timeSeriesAggregateExec`.\n\n3. **Error Handling**: The diff does not show any changes related to error handling. Ensure that the new method properly handles any potential exceptions that could arise from its parameters, especially if they can be null or invalid.\n\n4. **Performance Considerations**: The use of `stream().map().toList()` is generally fine, but if `groupSpecs` is large, consider whether this could lead to performance issues. If performance is critical, you might want to evaluate if a more efficient approach is necessary.\n\n5. **Documentation**: There are no comments or documentation for the new method. Adding JavaDoc comments to describe the purpose and parameters of `timeSeriesAggregatorOperatorFactory` would enhance maintainability.\n\n   - **Line 378**: Add JavaDoc comments for the new method.\n\n6. **Consistency in Naming Conventions**: Ensure that the naming conventions are consistent throughout the codebase. For example, if `OperatorFactory` is used in other parts of the code, ensure that similar naming conventions are followed.\n\nIn summary, while the changes improve the structure of the code, addressing the naming issues and enhancing documentation will improve clarity and maintainability. \n\n### Summary of Issues:\n- **Line 378**: Rename `timeSeriesAggregatorOperatorFactor` to `timeSeriesAggregatorOperatorFactory`.\n- **Line 378**: Consider renaming `ts` to `timeSeriesAggregateExec`.\n- **Error handling**: Ensure proper error handling in the new method.\n- **Performance**: Evaluate the performance of `stream().map().toList()` for large lists.\n- **Documentation**: Add JavaDoc comments for the new method.\n\nMake these adjustments to enhance the quality of the code.\n### x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/EsPhysicalOperationProviders.java\nOverall, the code diff appears to be well-structured and introduces a new operator factory for time series aggregation. However, there are a few areas that could benefit from improvements or clarifications.\n\n1. **Method Name Typo**: \n   - **Line 299**: The method name `timeSeriesAggregatorOperatorFactor` seems to contain a typo. It should likely be `timeSeriesAggregatorOperatorFactory` to maintain consistency with naming conventions and avoid confusion.\n\n2. **Nullability Considerations**:\n   - **Line 303**: The parameters `aggregatorFactories` and `groupSpecs` are not annotated for nullability. If these lists can be null, it would be prudent to add null checks or annotations to prevent potential `NullPointerExceptions`.\n\n3. **Performance Considerations**:\n   - **Line 303**: The method creates a new `TimeSeriesAggregationOperator.Factory` instance. If this method is called frequently, consider whether the creation of this factory can be optimized or cached, especially if the parameters do not change often.\n\n4. **Documentation**:\n   - There are no comments or JavaDoc for the new method `timeSeriesAggregatorOperatorFactor`. Adding documentation would help future maintainers understand the purpose and usage of this method, especially regarding the parameters and expected behavior.\n\n5. **Edge Cases**:\n   - Ensure that the logic within the `TimeSeriesAggregationOperator.Factory` handles edge cases, such as empty `aggregatorFactories` or `groupSpecs`. It would be beneficial to validate these inputs before passing them to the factory.\n\n6. **Error Handling**:\n   - Consider adding error handling or logging for unexpected states, especially if the method is part of a larger processing pipeline. This can help in debugging issues that arise during execution.\n\nIn summary, while the code introduces a useful feature, addressing the above points will enhance its robustness, maintainability, and clarity.\n### x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/PlannerUtils.java\nOverall, the code diff introduces a new method `requireTimeSeriesSource` that checks if any part of a `PhysicalPlan` contains a time series source. The implementation appears to be straightforward, but there are a few points to consider for improvement.\n\n1. **Bugs or Logical Errors**:\n   - There are no apparent bugs or logical errors in the new method. The use of pattern matching with `instanceof` is valid and correctly checks for the required conditions.\n\n2. **Performance Issues**:\n   - The method uses `anyMatch`, which is efficient for short-circuiting as soon as a match is found. However, if `plan` can be large, consider whether the structure of `PhysicalPlan` and its fragments is optimized for frequent checks like this. If this method is called often, profiling may be necessary to ensure performance is acceptable.\n\n3. **Security Vulnerabilities**:\n   - There are no security vulnerabilities evident in this code. The method does not expose any sensitive data or perform operations that could lead to security issues.\n\n4. **Code Style and Best Practices**:\n   - The code follows Java conventions well. However, consider adding JavaDoc comments to the new method to explain its purpose and usage. This will improve maintainability and clarity for future developers.\n\n5. **Potential Edge Cases**:\n   - The method assumes that `plan` and its fragments are not null. If there is a possibility of `plan` being null, it would be prudent to add a null check at the beginning of the method to avoid potential `NullPointerExceptions`.\n\n### Suggested Improvement:\n- Add a null check for `plan`:\n  ```java\n  if (plan == null) {\n      throw new IllegalArgumentException(\"PhysicalPlan cannot be null\");\n  }\n  ```\n\n### Conclusion:\nThe new method is a solid addition, but it could benefit from a null check and documentation. \n\nOverall, the code looks good with minor suggestions for improvement. \n\n**LGTM! (Looks Good To Me)**\n### x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/planner/TestPhysicalOperationProviders.java\nOverall, the code diff appears to be well-structured and follows good practices. However, there are a few areas that could benefit from further attention or clarification.\n\n1. **Method Naming**:\n   - **Line 156**: The method `timeSeriesAggregatorOperatorFactor` seems to have a typo in its name. It should likely be `timeSeriesAggregatorOperatorFactory` to maintain consistency with the naming convention of factory methods. This could lead to confusion for anyone reading the code.\n\n2. **Error Handling**:\n   - **Line 156**: There is no error handling or validation for the parameters passed to the `timeSeriesAggregatorOperatorFactory` method. It would be prudent to check if `aggregatorFactories` and `groupSpecs` are not null or empty before proceeding to create the `TimeSeriesAggregationOperator.Factory`. This can prevent potential `NullPointerExceptions` or unexpected behavior.\n\n3. **Documentation**:\n   - There are no comments or documentation for the new method `timeSeriesAggregatorOperatorFactory`. Adding JavaDoc comments to describe the method's purpose, parameters, and return value would improve code readability and maintainability.\n\n4. **Performance Considerations**:\n   - Depending on the implementation of `TimeSeriesAggregationOperator.Factory`, consider whether the parameters being passed (especially `groupSpecs` and `aggregatorFactories`) could lead to performance issues if they are large collections. If so, consider whether they should be processed or validated before being passed to the factory.\n\n5. **Code Style**:\n   - Ensure that the formatting is consistent with the rest of the codebase. For example, check the indentation and spacing around the method parameters to match the style used in other methods.\n\n6. **Potential Edge Cases**:\n   - Consider what happens if `context.pageSize(ts.estimatedRowSize())` returns an unexpected value (e.g., zero or negative). It would be beneficial to validate this value before using it in the factory instantiation.\n\nIn summary, while the code looks good overall, addressing the naming inconsistency, adding error handling, improving documentation, and considering performance and edge cases will enhance the quality and robustness of the implementation.\n\n\n---\n*This review was automatically generated by an AI assistant.*",
      "created_at": "2025-04-29T01:23:28",
      "file_reviews": [
        {
          "id": 90,
          "review_id": 9,
          "pr_id": 2487253010,
          "filename": "x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/aggregation/blockhash/TimeSeriesBlockHash.java",
          "content": "Overall, the code diff shows a significant refactor of the `TimeSeriesBlockHash` class, introducing new methods and improving the handling of timestamp and tsid data. However, there are a few areas that could be improved or warrant further attention.\n\n### Specific Issues:\n\n1. **Assertions Handling (Lines 43-45)**:\n   - The use of `Assertions.ENABLED` to conditionally create `assertingHash` is good for performance, but ensure that the assertions are properly configured in the build environment. If assertions are disabled in production, the `assertingHash` will be `null`, which could lead to `NullPointerException` if not handled correctly in the `assertingAddInputPage` method.\n\n2. **Error Handling (Lines 66-68)**:\n   - The `IllegalStateException` messages in `getTsidVector` and `getTimestampVector` methods are clear, but consider logging these exceptions for better traceability in production environments. This can help in debugging issues when the expected data types are not met.\n\n3. **Performance Considerations (Lines 83-84)**:\n   - The loop that processes `tsidOrdinals` could be optimized further. The current implementation checks for `newGroup` and `lastOrd` in every iteration. If the data is large, this could lead to performance bottlenecks. Consider batching or reducing the number of checks if possible.\n\n4. **Code Style and Readability (Lines 83-84)**:\n   - The nested structure of the `if` statements in the `add` method could be simplified for better readability. Consider extracting some of the logic into helper methods to reduce complexity and improve maintainability.\n\n5. **Potential Edge Cases (Lines 83-84)**:\n   - The logic for determining `newGroup` relies heavily on the order of the input data. Ensure that the input data is always sorted as expected. If there are any cases where the data might not be sorted, this could lead to incorrect behavior.\n\n6. **Unused Imports (Line 20)**:\n   - The import statement for `java.util.Objects` is no longer needed since the code now uses `Objects.requireNonNull` only in the commented-out section. Consider removing it to keep the code clean.\n\n7. **Closing Resources (Line 66)**:\n   - In the `close` method, ensure that `assertingHash` is checked for null before attempting to close it. This will prevent potential `NullPointerException` if assertions are disabled.\n\n### Conclusion:\nWhile the changes introduce useful functionality and improve the structure of the code, attention should be paid to error handling, performance optimizations, and ensuring that the code remains maintainable and readable. Addressing the above points will enhance the robustness and clarity of the implementation.",
          "created_at": "2025-04-29T01:23:28"
        },
        {
          "id": 91,
          "review_id": 9,
          "pr_id": 2487253010,
          "filename": "x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/data/OrdinalBytesRefBlock.java",
          "content": "Overall Assessment:\nThe code diff shows a modification in the return type of the `asVector` method from `BytesRefVector` to `OrdinalBytesRefVector`. This change appears to be a refactoring to provide a more specific type, which can improve type safety and clarity. However, it is essential to ensure that this change does not introduce any bugs or logical errors in the surrounding code.\n\nSpecific Issues:\n\n1. **Line 7**: \n   - **Change in Return Type**: Ensure that all calls to `asVector()` in the codebase are updated to handle the new return type `OrdinalBytesRefVector`. If there are any existing references to `BytesRefVector`, they may lead to compilation errors or runtime issues if not updated accordingly.\n\n2. **Type Compatibility**: \n   - Verify that `OrdinalBytesRefVector` is a subclass or compatible with `BytesRefVector`. If it is not, any code that expects a `BytesRefVector` will break. This could lead to runtime exceptions if not handled properly.\n\n3. **Testing**: \n   - Ensure that there are adequate unit tests covering the `asVector()` method. If the method's behavior has changed due to the new return type, additional tests may be necessary to validate the new functionality.\n\n4. **Documentation**: \n   - If there is any documentation or comments that reference `asVector()` returning a `BytesRefVector`, these should be updated to reflect the new return type. This helps maintain clarity for future developers.\n\n5. **Edge Cases**: \n   - Consider any edge cases that may arise from the new vector type. For instance, if `OrdinalBytesRefVector` has different handling for null or empty states compared to `BytesRefVector`, ensure that these cases are tested.\n\nIn summary, while the change appears to be a positive refactor, it is crucial to ensure that all dependent code is updated accordingly, and that thorough testing is conducted to prevent any potential issues.",
          "created_at": "2025-04-29T01:23:28"
        },
        {
          "id": 92,
          "review_id": 9,
          "pr_id": 2487253010,
          "filename": "x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/operator/TimeSeriesAggregationOperator.java",
          "content": "Overall, the code diff appears to be a modification that introduces a new boolean parameter `sortedInput` to the `Factory` record and adjusts the logic in the `get` method accordingly. The changes seem to be aimed at optimizing the aggregation process based on whether the input is sorted. However, there are a few points to consider:\n\n1. **Logical Errors**:\n   - **Line 38-40**: The logic for using `TimeSeriesBlockHash` is contingent on `sortedInput` being true and `groups.size() == 2`. Ensure that this condition is valid for all expected use cases. If `groups.size()` can be less than 2, it may lead to unexpected behavior or exceptions.\n\n2. **Performance Issues**:\n   - **Line 38-40**: The new logic introduces a conditional check for `sortedInput` and `groups.size()`. While this is a good optimization, ensure that the performance gain from using `TimeSeriesBlockHash` outweighs the overhead of the conditional check. It might be beneficial to benchmark both paths to confirm the performance improvement.\n\n3. **Code Style and Best Practices**:\n   - **Line 38-40**: Consider adding comments to clarify why `TimeSeriesBlockHash` is used when `sortedInput` is true and `groups.size() == 2`. This will help future maintainers understand the rationale behind this decision.\n   - **Line 38-40**: The inline lambda for the block hash creation could be extracted into a separate method for better readability and maintainability.\n\n4. **Potential Edge Cases**:\n   - **Line 38-40**: If `groups.size()` is less than 2, the current implementation defaults to using `BlockHash.build`. Ensure that this fallback is appropriate and does not lead to incorrect behavior in scenarios where fewer than two groups are provided.\n   - **Line 38-40**: Consider what happens if `sortedInput` is true but the input data is not actually sorted. There should be validation or documentation to ensure that users of this class understand the requirements for `sortedInput`.\n\nIn summary, while the changes seem to be on the right track, careful consideration of edge cases and performance implications is necessary. Adding comments and possibly refactoring for clarity would also enhance the code quality.",
          "created_at": "2025-04-29T01:23:28"
        },
        {
          "id": 93,
          "review_id": 9,
          "pr_id": 2487253010,
          "filename": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/AbstractPhysicalOperationProviders.java",
          "content": "Overall, the code diff appears to be a refactoring of the `AbstractPhysicalOperationProviders` class, specifically focusing on the handling of time-series aggregation. The changes seem to improve the clarity of the code by introducing a new method for creating the time series aggregator operator factory. However, there are a few points worth noting:\n\n1. **Code Clarity**: The new method `timeSeriesAggregatorOperatorFactor` is introduced, which is a good practice for improving code readability. However, the method name has a typo; it should be `timeSeriesAggregatorOperatorFactory` instead of `timeSeriesAggregatorOperatorFactor`. This could lead to confusion.\n\n   - **Line 378**: Rename `timeSeriesAggregatorOperatorFactor` to `timeSeriesAggregatorOperatorFactory`.\n\n2. **Parameter Naming**: The parameter names in the new method could be more descriptive. For example, `ts` could be renamed to `timeSeriesAggregateExec` for better clarity.\n\n   - **Line 378**: Consider renaming `ts` to `timeSeriesAggregateExec`.\n\n3. **Error Handling**: The diff does not show any changes related to error handling. Ensure that the new method properly handles any potential exceptions that could arise from its parameters, especially if they can be null or invalid.\n\n4. **Performance Considerations**: The use of `stream().map().toList()` is generally fine, but if `groupSpecs` is large, consider whether this could lead to performance issues. If performance is critical, you might want to evaluate if a more efficient approach is necessary.\n\n5. **Documentation**: There are no comments or documentation for the new method. Adding JavaDoc comments to describe the purpose and parameters of `timeSeriesAggregatorOperatorFactory` would enhance maintainability.\n\n   - **Line 378**: Add JavaDoc comments for the new method.\n\n6. **Consistency in Naming Conventions**: Ensure that the naming conventions are consistent throughout the codebase. For example, if `OperatorFactory` is used in other parts of the code, ensure that similar naming conventions are followed.\n\nIn summary, while the changes improve the structure of the code, addressing the naming issues and enhancing documentation will improve clarity and maintainability. \n\n### Summary of Issues:\n- **Line 378**: Rename `timeSeriesAggregatorOperatorFactor` to `timeSeriesAggregatorOperatorFactory`.\n- **Line 378**: Consider renaming `ts` to `timeSeriesAggregateExec`.\n- **Error handling**: Ensure proper error handling in the new method.\n- **Performance**: Evaluate the performance of `stream().map().toList()` for large lists.\n- **Documentation**: Add JavaDoc comments for the new method.\n\nMake these adjustments to enhance the quality of the code.",
          "created_at": "2025-04-29T01:23:28"
        },
        {
          "id": 94,
          "review_id": 9,
          "pr_id": 2487253010,
          "filename": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/EsPhysicalOperationProviders.java",
          "content": "Overall, the code diff appears to be well-structured and introduces a new operator factory for time series aggregation. However, there are a few areas that could benefit from improvements or clarifications.\n\n1. **Method Name Typo**: \n   - **Line 299**: The method name `timeSeriesAggregatorOperatorFactor` seems to contain a typo. It should likely be `timeSeriesAggregatorOperatorFactory` to maintain consistency with naming conventions and avoid confusion.\n\n2. **Nullability Considerations**:\n   - **Line 303**: The parameters `aggregatorFactories` and `groupSpecs` are not annotated for nullability. If these lists can be null, it would be prudent to add null checks or annotations to prevent potential `NullPointerExceptions`.\n\n3. **Performance Considerations**:\n   - **Line 303**: The method creates a new `TimeSeriesAggregationOperator.Factory` instance. If this method is called frequently, consider whether the creation of this factory can be optimized or cached, especially if the parameters do not change often.\n\n4. **Documentation**:\n   - There are no comments or JavaDoc for the new method `timeSeriesAggregatorOperatorFactor`. Adding documentation would help future maintainers understand the purpose and usage of this method, especially regarding the parameters and expected behavior.\n\n5. **Edge Cases**:\n   - Ensure that the logic within the `TimeSeriesAggregationOperator.Factory` handles edge cases, such as empty `aggregatorFactories` or `groupSpecs`. It would be beneficial to validate these inputs before passing them to the factory.\n\n6. **Error Handling**:\n   - Consider adding error handling or logging for unexpected states, especially if the method is part of a larger processing pipeline. This can help in debugging issues that arise during execution.\n\nIn summary, while the code introduces a useful feature, addressing the above points will enhance its robustness, maintainability, and clarity.",
          "created_at": "2025-04-29T01:23:28"
        },
        {
          "id": 95,
          "review_id": 9,
          "pr_id": 2487253010,
          "filename": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/PlannerUtils.java",
          "content": "Overall, the code diff introduces a new method `requireTimeSeriesSource` that checks if any part of a `PhysicalPlan` contains a time series source. The implementation appears to be straightforward, but there are a few points to consider for improvement.\n\n1. **Bugs or Logical Errors**:\n   - There are no apparent bugs or logical errors in the new method. The use of pattern matching with `instanceof` is valid and correctly checks for the required conditions.\n\n2. **Performance Issues**:\n   - The method uses `anyMatch`, which is efficient for short-circuiting as soon as a match is found. However, if `plan` can be large, consider whether the structure of `PhysicalPlan` and its fragments is optimized for frequent checks like this. If this method is called often, profiling may be necessary to ensure performance is acceptable.\n\n3. **Security Vulnerabilities**:\n   - There are no security vulnerabilities evident in this code. The method does not expose any sensitive data or perform operations that could lead to security issues.\n\n4. **Code Style and Best Practices**:\n   - The code follows Java conventions well. However, consider adding JavaDoc comments to the new method to explain its purpose and usage. This will improve maintainability and clarity for future developers.\n\n5. **Potential Edge Cases**:\n   - The method assumes that `plan` and its fragments are not null. If there is a possibility of `plan` being null, it would be prudent to add a null check at the beginning of the method to avoid potential `NullPointerExceptions`.\n\n### Suggested Improvement:\n- Add a null check for `plan`:\n  ```java\n  if (plan == null) {\n      throw new IllegalArgumentException(\"PhysicalPlan cannot be null\");\n  }\n  ```\n\n### Conclusion:\nThe new method is a solid addition, but it could benefit from a null check and documentation. \n\nOverall, the code looks good with minor suggestions for improvement. \n\n**LGTM! (Looks Good To Me)**",
          "created_at": "2025-04-29T01:23:28"
        },
        {
          "id": 96,
          "review_id": 9,
          "pr_id": 2487253010,
          "filename": "x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/planner/TestPhysicalOperationProviders.java",
          "content": "Overall, the code diff appears to be well-structured and follows good practices. However, there are a few areas that could benefit from further attention or clarification.\n\n1. **Method Naming**:\n   - **Line 156**: The method `timeSeriesAggregatorOperatorFactor` seems to have a typo in its name. It should likely be `timeSeriesAggregatorOperatorFactory` to maintain consistency with the naming convention of factory methods. This could lead to confusion for anyone reading the code.\n\n2. **Error Handling**:\n   - **Line 156**: There is no error handling or validation for the parameters passed to the `timeSeriesAggregatorOperatorFactory` method. It would be prudent to check if `aggregatorFactories` and `groupSpecs` are not null or empty before proceeding to create the `TimeSeriesAggregationOperator.Factory`. This can prevent potential `NullPointerExceptions` or unexpected behavior.\n\n3. **Documentation**:\n   - There are no comments or documentation for the new method `timeSeriesAggregatorOperatorFactory`. Adding JavaDoc comments to describe the method's purpose, parameters, and return value would improve code readability and maintainability.\n\n4. **Performance Considerations**:\n   - Depending on the implementation of `TimeSeriesAggregationOperator.Factory`, consider whether the parameters being passed (especially `groupSpecs` and `aggregatorFactories`) could lead to performance issues if they are large collections. If so, consider whether they should be processed or validated before being passed to the factory.\n\n5. **Code Style**:\n   - Ensure that the formatting is consistent with the rest of the codebase. For example, check the indentation and spacing around the method parameters to match the style used in other methods.\n\n6. **Potential Edge Cases**:\n   - Consider what happens if `context.pageSize(ts.estimatedRowSize())` returns an unexpected value (e.g., zero or negative). It would be beneficial to validate this value before using it in the factory instantiation.\n\nIn summary, while the code looks good overall, addressing the naming inconsistency, adding error handling, improving documentation, and considering performance and edge cases will enhance the quality and robustness of the implementation.",
          "created_at": "2025-04-29T01:23:28"
        }
      ]
    },
    {
      "id": 2,
      "pr_id": 2487253010,
      "summary": "This pull request (PR) titled \"Enable time-series block hash\" introduces enhancements to the Elasticsearch ESQL plugin, specifically focusing on time-series data processing. The primary purpose of these changes is to implement a new mechanism for calculating block hashes in time-series data, which can improve performance and data integrity during aggregation operations.\n\nSignificant modifications include the addition of 113 lines and deletion of 28 lines in the `TimeSeriesBlockHash.java` file, which likely contains the core logic for the new hashing mechanism. Other files, such as `TimeSeriesAggregationOperator.java` and `AbstractPhysicalOperationProviders.java`, have also been updated to integrate this new functionality, with changes that suggest improvements in how time-series data is handled during aggregation.\n\nPotential concerns include the need for thorough testing to ensure that the new block hash implementation does not introduce regressions or performance issues, especially given the complexity of time-series data. Additionally, the impact on existing aggregation operations should be evaluated to confirm that the new logic aligns with expected behaviors. It may also be beneficial to include documentation or comments within the code to clarify the rationale behind the changes for future maintainers. Overall, the PR appears to be a valuable enhancement to the ESQL plugin, but careful review and testing are recommended.",
      "full_review": "# AI Review \ud83e\udd16\n\n## Summary\nThis pull request (PR) titled \"Enable time-series block hash\" introduces enhancements to the Elasticsearch ESQL plugin, specifically focusing on time-series data processing. The primary purpose of these changes is to implement a new mechanism for calculating block hashes in time-series data, which can improve performance and data integrity during aggregation operations.\n\nSignificant modifications include the addition of 113 lines and deletion of 28 lines in the `TimeSeriesBlockHash.java` file, which likely contains the core logic for the new hashing mechanism. Other files, such as `TimeSeriesAggregationOperator.java` and `AbstractPhysicalOperationProviders.java`, have also been updated to integrate this new functionality, with changes that suggest improvements in how time-series data is handled during aggregation.\n\nPotential concerns include the need for thorough testing to ensure that the new block hash implementation does not introduce regressions or performance issues, especially given the complexity of time-series data. Additionally, the impact on existing aggregation operations should be evaluated to confirm that the new logic aligns with expected behaviors. It may also be beneficial to include documentation or comments within the code to clarify the rationale behind the changes for future maintainers. Overall, the PR appears to be a valuable enhancement to the ESQL plugin, but careful review and testing are recommended.\n\n## Detailed Review\n\n### x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/aggregation/blockhash/TimeSeriesBlockHash.java\nOverall, the code diff shows significant improvements in the `TimeSeriesBlockHash` class, particularly in how it handles the addition of timestamps and the management of groups. However, there are a few areas that could benefit from further attention regarding performance, code clarity, and potential edge cases.\n\n### Specific Issues:\n\n1. **Assertions in Production Code (Line 43-45)**:\n   - The use of assertions to check for conditions that should always be true in production code can lead to unexpected behavior if assertions are disabled. Consider using proper error handling (e.g., throwing exceptions) instead of relying solely on assertions for critical checks.\n\n2. **Potential Null Pointer Exception (Line 120)**:\n   - The `assertingHash` is initialized to `null` when assertions are disabled. However, in the `assertingAddInputPage` method, there is an assertion that `assertingHash` is not null. This could lead to a `NullPointerException` if assertions are disabled. Ensure that the logic accounts for this scenario.\n\n3. **Inefficient Grouping Logic (Line 90-100)**:\n   - The logic for determining if a new group has started could be optimized. The current implementation checks multiple conditions which may lead to unnecessary complexity. Consider simplifying the logic to improve readability and performance.\n\n4. **Error Handling in `getTsidVector` and `getTimestampVector` (Line 66-78)**:\n   - The methods throw `IllegalStateException` if the expected block types are not found. While this is acceptable, consider logging the error or providing more context in the exception message to aid debugging.\n\n5. **Code Duplication (Line 90-100)**:\n   - The logic for handling the `newGroup` condition is somewhat duplicated. Consider refactoring this logic into a separate method to improve maintainability and reduce code duplication.\n\n6. **Use of `final` Keyword (Line 90)**:\n   - The `final` keyword is used for local variables like `spare` and `newTsid`. While this is not incorrect, it may not be necessary for local variables unless you want to enforce immutability. Consider removing it for clarity unless it serves a specific purpose.\n\n7. **Commented TODO (Line 83)**:\n   - The TODO comment regarding optimizing the incoming ordinal block should be addressed or removed if it is no longer relevant. Leaving TODOs in the code can lead to technical debt if not tracked properly.\n\n### Conclusion:\nWhile the code shows improvements and adheres to many best practices, addressing the above points will enhance its robustness, performance, and maintainability. Consider implementing the suggested changes to ensure the code is both efficient and clear.\n### x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/data/OrdinalBytesRefBlock.java\nOverall Assessment:\nThe code diff shows a modification in the return type of the `asVector` method from `BytesRefVector` to `OrdinalBytesRefVector`. This change appears to be a refactoring to provide a more specific type, which can improve type safety and clarity. However, it is essential to ensure that this change does not introduce any bugs or logical errors.\n\nSpecific Issues:\n\n1. **Line 7**: \n   - **Change in Return Type**: The change from `BytesRefVector` to `OrdinalBytesRefVector` should be reviewed to ensure that all usages of `asVector()` in the codebase are compatible with the new return type. If there are any existing calls to this method expecting a `BytesRefVector`, it could lead to compilation errors or runtime issues.\n\n2. **Compatibility**: \n   - Ensure that `OrdinalBytesRefVector` is a subclass of `BytesRefVector` or implements the same interface. If it does not, any polymorphic behavior expected from `BytesRefVector` will be lost, potentially leading to runtime exceptions.\n\n3. **Documentation**: \n   - If this method is part of a public API, consider updating the method documentation to reflect the new return type. This will help other developers understand the change and its implications.\n\n4. **Testing**: \n   - Ensure that there are adequate unit tests covering the `asVector()` method. After changing the return type, it is crucial to verify that the functionality remains intact and that the new type behaves as expected.\n\n5. **Edge Cases**: \n   - Review how `OrdinalBytesRefVector` handles edge cases, such as null values or empty states. Ensure that the new implementation does not introduce any unexpected behavior in these scenarios.\n\nIn summary, while the change appears to be a positive refactor, it is essential to verify compatibility, update documentation, and ensure thorough testing to avoid potential issues.\n### x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/operator/TimeSeriesAggregationOperator.java\nOverall, the code diff introduces a new boolean parameter `sortedInput` to the `Factory` record and modifies the `get` method to conditionally use `TimeSeriesBlockHash` based on this parameter. The changes appear to be aimed at optimizing the aggregation process when the input is sorted. However, there are a few points to consider:\n\n1. **Logical Errors**:\n   - Line 40: The condition `if (sortedInput && groups.size() == 2)` assumes that the optimization is only valid when there are exactly two groups. This may not cover all valid cases where `TimeSeriesBlockHash` could be beneficial. Consider whether this condition should be expanded or clarified.\n\n2. **Performance Issues**:\n   - The use of `TimeSeriesBlockHash` is a good optimization, but ensure that the performance benefits are validated through benchmarking. If `TimeSeriesBlockHash` has a significant overhead when the input is not sorted, it might be worth considering additional checks or profiling to ensure that the performance gain is realized in practice.\n\n3. **Code Style and Best Practices**:\n   - The code is generally well-structured, but consider adding comments to explain the rationale behind the conditional logic for future maintainers. This will help clarify why `TimeSeriesBlockHash` is only used under specific conditions.\n\n4. **Potential Edge Cases**:\n   - Ensure that the `groups` list is not empty before accessing `groups.get(0)` and `groups.get(1)`. If `groups` can be empty or have fewer than two elements, this will lead to an `IndexOutOfBoundsException`. Adding a check for the size of `groups` before accessing its elements would be prudent.\n\n5. **Testing**:\n   - Ensure that there are adequate unit tests covering both scenarios: when `sortedInput` is true and when it is false. Additionally, tests should cover edge cases such as when `groups` has fewer than two elements.\n\nIn summary, while the changes are a step in the right direction, addressing the potential logical error regarding the `groups` size and ensuring robust testing will enhance the reliability and performance of the code.\n### x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/AbstractPhysicalOperationProviders.java\nOverall, the code diff appears to be a refactoring of the `AbstractPhysicalOperationProviders` class, specifically focusing on the handling of time-series aggregation. The changes seem to improve code organization by introducing a new method for time-series aggregation operator factory creation. However, there are a few points worth noting:\n\n1. **Code Clarity**: \n   - The new method `timeSeriesAggregatorOperatorFactor` is introduced, but the naming convention is slightly off. It should be `timeSeriesAggregatorOperatorFactory` to maintain consistency with the naming of other factory methods in the class. This will improve readability and maintainability.\n\n2. **Line 12**: \n   - The import statement for `TimeSeriesAggregationOperator` has been removed. Ensure that this class is not used elsewhere in the file. If it is still needed, its removal could lead to compilation errors.\n\n3. **Line 175**: \n   - The refactoring changes the way the `operatorFactory` is created. Ensure that the new method `timeSeriesAggregatorOperatorFactor` correctly handles all necessary parameters and that there are no changes in the logic that could lead to incorrect behavior. It would be beneficial to review the implementation of this new method to confirm it aligns with the previous logic.\n\n4. **Performance Considerations**: \n   - The use of `stream().map(...).toList()` is generally fine, but if `groupSpecs` is large, consider whether this could lead to performance issues. If the list is not needed in a specific order, using `Collectors.toCollection(ArrayList::new)` might be more efficient.\n\n5. **Edge Cases**: \n   - Ensure that the new method handles edge cases, such as empty lists for `aggregatorFactories` or `groupSpecs`. It would be prudent to add null checks or validations within the new method to prevent potential `NullPointerExceptions`.\n\n6. **Documentation**: \n   - Consider adding JavaDoc comments for the new method `timeSeriesAggregatorOperatorFactor` to explain its purpose, parameters, and expected behavior. This will help future developers understand the code better.\n\nIn summary, while the changes seem to improve the structure of the code, attention should be paid to naming conventions, potential performance implications, and ensuring that edge cases are handled properly. \n\n**Actionable Feedback**:\n- Rename `timeSeriesAggregatorOperatorFactor` to `timeSeriesAggregatorOperatorFactory`.\n- Verify the removal of `TimeSeriesAggregationOperator` is intentional and does not affect other parts of the code.\n- Review the implementation of `timeSeriesAggregatorOperatorFactor` for correctness.\n- Consider performance implications of using streams and handle edge cases in the new method.\n- Add documentation for the new method.\n\nOverall, the changes are on the right track but require some adjustments for clarity and robustness.\n### x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/EsPhysicalOperationProviders.java\nOverall, the code diff appears to be well-structured and introduces a new operator factory for time series aggregation. However, there are a few areas that could benefit from improvements or clarifications.\n\n1. **Method Name Typo**: \n   - **Line 299**: The method name `timeSeriesAggregatorOperatorFactor` seems to contain a typo. It should likely be `timeSeriesAggregatorOperatorFactory` to maintain consistency with naming conventions and avoid confusion.\n\n2. **Nullability Considerations**:\n   - **Line 303**: The parameters `aggregatorFactories` and `groupSpecs` are not annotated for nullability. If these lists can be null, it would be prudent to add `@Nullable` annotations or handle potential null values within the method to avoid `NullPointerException`.\n\n3. **Performance Considerations**:\n   - **Line 307**: The expression `shardContexts.size() == 1 && ts.anyMatch(p -> p instanceof TimeSeriesSourceExec)` could be optimized. If `shardContexts` is a large collection, calling `size()` may not be efficient. Consider using `isEmpty()` or checking the first element directly if applicable.\n\n4. **Code Style and Best Practices**:\n   - **Line 299-310**: The method is quite long and could benefit from additional comments explaining the purpose of each parameter and the overall functionality. This would improve readability and maintainability.\n   - Consider using consistent formatting for method parameters. For example, aligning the parameters vertically can enhance readability.\n\n5. **Edge Cases**:\n   - Ensure that the method handles edge cases where `aggregatorFactories` or `groupSpecs` might be empty. Depending on the implementation of `TimeSeriesAggregationOperator.Factory`, this could lead to unexpected behavior.\n\n6. **Testing**:\n   - Ensure that there are adequate unit tests covering this new method, especially for various combinations of input parameters, including edge cases like empty lists or null values.\n\nIn summary, while the code introduces a useful feature, addressing the above points will enhance its robustness and maintainability.\n### x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/PlannerUtils.java\nOverall, the code diff introduces a new method `requireTimeSeriesSource` that checks if any part of a `PhysicalPlan` contains a time series source. The implementation appears to be straightforward, but there are a few areas that could be improved or warrant further consideration.\n\n1. **Code Style and Best Practices**:\n   - **Line 159**: The use of the `instanceof` operator with pattern matching (e.g., `if (p instanceof FragmentExec f)`) is a nice modern Java feature. However, ensure that this is consistent with the project's Java version requirements. If the project supports Java 16 or higher, this is fine; otherwise, consider using the traditional casting approach for compatibility.\n   - **Line 161**: The nested `instanceof` check could be simplified by using a guard clause. Instead of returning `false` in the `else` block, you could return `false` immediately if `p` is not an instance of `FragmentExec`.\n\n2. **Performance Issues**:\n   - The method uses `anyMatch`, which is efficient for short-circuiting the evaluation. However, if `plan` can be large, consider whether the structure of `PhysicalPlan` allows for early exits or optimizations. If `plan` is frequently large, profiling may be necessary to ensure performance is acceptable.\n\n3. **Potential Edge Cases**:\n   - **Line 158**: If `plan` is `null`, this will throw a `NullPointerException`. Consider adding a null check at the beginning of the method to handle this gracefully.\n   - The method assumes that `FragmentExec` and `EsRelation` are well-defined and that `indexMode()` will not return `null`. If there's a possibility of `null` values, you should add checks to avoid potential `NullPointerExceptions`.\n\n4. **Documentation**:\n   - There are no comments or documentation for the new method. Adding a brief JavaDoc comment explaining the purpose of `requireTimeSeriesSource`, its parameters, and return value would improve code readability and maintainability.\n\n5. **Unit Tests**:\n   - Ensure that there are corresponding unit tests for this new method. Testing various scenarios, including edge cases (e.g., empty plans, plans with no `FragmentExec`, etc.), will help ensure the method behaves as expected.\n\nIn summary, while the new method is a useful addition, it could benefit from additional null checks, documentation, and consideration of edge cases. Addressing these points will enhance the robustness and maintainability of the code.\n\n**Actionable Feedback**:\n- Add a null check for `plan` at the start of `requireTimeSeriesSource`.\n- Consider using a guard clause for the `else` statement.\n- Add JavaDoc comments for the new method.\n- Ensure unit tests cover various scenarios for the new method.\n\nOverall, the implementation is on the right track but could be improved with these considerations.\n### x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/planner/TestPhysicalOperationProviders.java\nOverall, the code diff appears to be a well-structured addition to the `TestPhysicalOperationProviders` class, introducing a new method for handling time series aggregation. However, there are a few points worth noting for improvement and clarity.\n\n1. **Method Naming**:\n   - **Line 156**: The method name `timeSeriesAggregatorOperatorFactor` seems to be a typo. It should likely be `timeSeriesAggregatorOperatorFactory` to maintain consistency with the naming convention of factory methods. This could lead to confusion for anyone reading the code.\n\n2. **Parameter Validation**:\n   - **Line 157-162**: There is no validation for the parameters being passed into the `timeSeriesAggregatorOperatorFactory` method. It would be prudent to check if `ts`, `aggregatorMode`, `aggregatorFactories`, and `groupSpecs` are not null before proceeding. This can prevent potential `NullPointerExceptions` during runtime.\n\n3. **Documentation**:\n   - There are no comments or documentation for the new method. Adding JavaDoc comments to describe the purpose of the method, its parameters, and return value would enhance code readability and maintainability.\n\n4. **Performance Considerations**:\n   - **Line 162**: The method creates a new instance of `TimeSeriesAggregationOperator.Factory`. Depending on how often this method is called, consider whether this factory instantiation could be optimized or reused to avoid unnecessary object creation.\n\n5. **Edge Cases**:\n   - Consider what happens if `context.pageSize(ts.estimatedRowSize())` returns an unexpected value (e.g., zero or negative). It would be beneficial to handle such cases to avoid runtime errors.\n\n6. **Code Style**:\n   - Ensure consistent formatting and spacing throughout the code. For example, there is a mix of spaces and tabs in the surrounding code. Consistency in indentation style improves readability.\n\nIn summary, while the new method adds functionality, addressing the naming issue, adding parameter validation, improving documentation, and considering performance and edge cases will enhance the quality of the code.\n\n\n---\n*This review was automatically generated by an AI assistant.*",
      "created_at": "2025-04-29T01:07:52",
      "file_reviews": [
        {
          "id": 23,
          "review_id": 2,
          "pr_id": 2487253010,
          "filename": "x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/aggregation/blockhash/TimeSeriesBlockHash.java",
          "content": "Overall, the code diff shows significant improvements in the `TimeSeriesBlockHash` class, particularly in how it handles the addition of timestamps and the management of groups. However, there are a few areas that could benefit from further attention regarding performance, code clarity, and potential edge cases.\n\n### Specific Issues:\n\n1. **Assertions in Production Code (Line 43-45)**:\n   - The use of assertions to check for conditions that should always be true in production code can lead to unexpected behavior if assertions are disabled. Consider using proper error handling (e.g., throwing exceptions) instead of relying solely on assertions for critical checks.\n\n2. **Potential Null Pointer Exception (Line 120)**:\n   - The `assertingHash` is initialized to `null` when assertions are disabled. However, in the `assertingAddInputPage` method, there is an assertion that `assertingHash` is not null. This could lead to a `NullPointerException` if assertions are disabled. Ensure that the logic accounts for this scenario.\n\n3. **Inefficient Grouping Logic (Line 90-100)**:\n   - The logic for determining if a new group has started could be optimized. The current implementation checks multiple conditions which may lead to unnecessary complexity. Consider simplifying the logic to improve readability and performance.\n\n4. **Error Handling in `getTsidVector` and `getTimestampVector` (Line 66-78)**:\n   - The methods throw `IllegalStateException` if the expected block types are not found. While this is acceptable, consider logging the error or providing more context in the exception message to aid debugging.\n\n5. **Code Duplication (Line 90-100)**:\n   - The logic for handling the `newGroup` condition is somewhat duplicated. Consider refactoring this logic into a separate method to improve maintainability and reduce code duplication.\n\n6. **Use of `final` Keyword (Line 90)**:\n   - The `final` keyword is used for local variables like `spare` and `newTsid`. While this is not incorrect, it may not be necessary for local variables unless you want to enforce immutability. Consider removing it for clarity unless it serves a specific purpose.\n\n7. **Commented TODO (Line 83)**:\n   - The TODO comment regarding optimizing the incoming ordinal block should be addressed or removed if it is no longer relevant. Leaving TODOs in the code can lead to technical debt if not tracked properly.\n\n### Conclusion:\nWhile the code shows improvements and adheres to many best practices, addressing the above points will enhance its robustness, performance, and maintainability. Consider implementing the suggested changes to ensure the code is both efficient and clear.",
          "created_at": "2025-04-29T01:07:52"
        },
        {
          "id": 24,
          "review_id": 2,
          "pr_id": 2487253010,
          "filename": "x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/data/OrdinalBytesRefBlock.java",
          "content": "Overall Assessment:\nThe code diff shows a modification in the return type of the `asVector` method from `BytesRefVector` to `OrdinalBytesRefVector`. This change appears to be a refactoring to provide a more specific type, which can improve type safety and clarity. However, it is essential to ensure that this change does not introduce any bugs or logical errors.\n\nSpecific Issues:\n\n1. **Line 7**: \n   - **Change in Return Type**: The change from `BytesRefVector` to `OrdinalBytesRefVector` should be reviewed to ensure that all usages of `asVector()` in the codebase are compatible with the new return type. If there are any existing calls to this method expecting a `BytesRefVector`, it could lead to compilation errors or runtime issues.\n\n2. **Compatibility**: \n   - Ensure that `OrdinalBytesRefVector` is a subclass of `BytesRefVector` or implements the same interface. If it does not, any polymorphic behavior expected from `BytesRefVector` will be lost, potentially leading to runtime exceptions.\n\n3. **Documentation**: \n   - If this method is part of a public API, consider updating the method documentation to reflect the new return type. This will help other developers understand the change and its implications.\n\n4. **Testing**: \n   - Ensure that there are adequate unit tests covering the `asVector()` method. After changing the return type, it is crucial to verify that the functionality remains intact and that the new type behaves as expected.\n\n5. **Edge Cases**: \n   - Review how `OrdinalBytesRefVector` handles edge cases, such as null values or empty states. Ensure that the new implementation does not introduce any unexpected behavior in these scenarios.\n\nIn summary, while the change appears to be a positive refactor, it is essential to verify compatibility, update documentation, and ensure thorough testing to avoid potential issues.",
          "created_at": "2025-04-29T01:07:52"
        },
        {
          "id": 25,
          "review_id": 2,
          "pr_id": 2487253010,
          "filename": "x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/operator/TimeSeriesAggregationOperator.java",
          "content": "Overall, the code diff introduces a new boolean parameter `sortedInput` to the `Factory` record and modifies the `get` method to conditionally use `TimeSeriesBlockHash` based on this parameter. The changes appear to be aimed at optimizing the aggregation process when the input is sorted. However, there are a few points to consider:\n\n1. **Logical Errors**:\n   - Line 40: The condition `if (sortedInput && groups.size() == 2)` assumes that the optimization is only valid when there are exactly two groups. This may not cover all valid cases where `TimeSeriesBlockHash` could be beneficial. Consider whether this condition should be expanded or clarified.\n\n2. **Performance Issues**:\n   - The use of `TimeSeriesBlockHash` is a good optimization, but ensure that the performance benefits are validated through benchmarking. If `TimeSeriesBlockHash` has a significant overhead when the input is not sorted, it might be worth considering additional checks or profiling to ensure that the performance gain is realized in practice.\n\n3. **Code Style and Best Practices**:\n   - The code is generally well-structured, but consider adding comments to explain the rationale behind the conditional logic for future maintainers. This will help clarify why `TimeSeriesBlockHash` is only used under specific conditions.\n\n4. **Potential Edge Cases**:\n   - Ensure that the `groups` list is not empty before accessing `groups.get(0)` and `groups.get(1)`. If `groups` can be empty or have fewer than two elements, this will lead to an `IndexOutOfBoundsException`. Adding a check for the size of `groups` before accessing its elements would be prudent.\n\n5. **Testing**:\n   - Ensure that there are adequate unit tests covering both scenarios: when `sortedInput` is true and when it is false. Additionally, tests should cover edge cases such as when `groups` has fewer than two elements.\n\nIn summary, while the changes are a step in the right direction, addressing the potential logical error regarding the `groups` size and ensuring robust testing will enhance the reliability and performance of the code.",
          "created_at": "2025-04-29T01:07:52"
        },
        {
          "id": 26,
          "review_id": 2,
          "pr_id": 2487253010,
          "filename": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/AbstractPhysicalOperationProviders.java",
          "content": "Overall, the code diff appears to be a refactoring of the `AbstractPhysicalOperationProviders` class, specifically focusing on the handling of time-series aggregation. The changes seem to improve code organization by introducing a new method for time-series aggregation operator factory creation. However, there are a few points worth noting:\n\n1. **Code Clarity**: \n   - The new method `timeSeriesAggregatorOperatorFactor` is introduced, but the naming convention is slightly off. It should be `timeSeriesAggregatorOperatorFactory` to maintain consistency with the naming of other factory methods in the class. This will improve readability and maintainability.\n\n2. **Line 12**: \n   - The import statement for `TimeSeriesAggregationOperator` has been removed. Ensure that this class is not used elsewhere in the file. If it is still needed, its removal could lead to compilation errors.\n\n3. **Line 175**: \n   - The refactoring changes the way the `operatorFactory` is created. Ensure that the new method `timeSeriesAggregatorOperatorFactor` correctly handles all necessary parameters and that there are no changes in the logic that could lead to incorrect behavior. It would be beneficial to review the implementation of this new method to confirm it aligns with the previous logic.\n\n4. **Performance Considerations**: \n   - The use of `stream().map(...).toList()` is generally fine, but if `groupSpecs` is large, consider whether this could lead to performance issues. If the list is not needed in a specific order, using `Collectors.toCollection(ArrayList::new)` might be more efficient.\n\n5. **Edge Cases**: \n   - Ensure that the new method handles edge cases, such as empty lists for `aggregatorFactories` or `groupSpecs`. It would be prudent to add null checks or validations within the new method to prevent potential `NullPointerExceptions`.\n\n6. **Documentation**: \n   - Consider adding JavaDoc comments for the new method `timeSeriesAggregatorOperatorFactor` to explain its purpose, parameters, and expected behavior. This will help future developers understand the code better.\n\nIn summary, while the changes seem to improve the structure of the code, attention should be paid to naming conventions, potential performance implications, and ensuring that edge cases are handled properly. \n\n**Actionable Feedback**:\n- Rename `timeSeriesAggregatorOperatorFactor` to `timeSeriesAggregatorOperatorFactory`.\n- Verify the removal of `TimeSeriesAggregationOperator` is intentional and does not affect other parts of the code.\n- Review the implementation of `timeSeriesAggregatorOperatorFactor` for correctness.\n- Consider performance implications of using streams and handle edge cases in the new method.\n- Add documentation for the new method.\n\nOverall, the changes are on the right track but require some adjustments for clarity and robustness.",
          "created_at": "2025-04-29T01:07:52"
        },
        {
          "id": 27,
          "review_id": 2,
          "pr_id": 2487253010,
          "filename": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/EsPhysicalOperationProviders.java",
          "content": "Overall, the code diff appears to be well-structured and introduces a new operator factory for time series aggregation. However, there are a few areas that could benefit from improvements or clarifications.\n\n1. **Method Name Typo**: \n   - **Line 299**: The method name `timeSeriesAggregatorOperatorFactor` seems to contain a typo. It should likely be `timeSeriesAggregatorOperatorFactory` to maintain consistency with naming conventions and avoid confusion.\n\n2. **Nullability Considerations**:\n   - **Line 303**: The parameters `aggregatorFactories` and `groupSpecs` are not annotated for nullability. If these lists can be null, it would be prudent to add `@Nullable` annotations or handle potential null values within the method to avoid `NullPointerException`.\n\n3. **Performance Considerations**:\n   - **Line 307**: The expression `shardContexts.size() == 1 && ts.anyMatch(p -> p instanceof TimeSeriesSourceExec)` could be optimized. If `shardContexts` is a large collection, calling `size()` may not be efficient. Consider using `isEmpty()` or checking the first element directly if applicable.\n\n4. **Code Style and Best Practices**:\n   - **Line 299-310**: The method is quite long and could benefit from additional comments explaining the purpose of each parameter and the overall functionality. This would improve readability and maintainability.\n   - Consider using consistent formatting for method parameters. For example, aligning the parameters vertically can enhance readability.\n\n5. **Edge Cases**:\n   - Ensure that the method handles edge cases where `aggregatorFactories` or `groupSpecs` might be empty. Depending on the implementation of `TimeSeriesAggregationOperator.Factory`, this could lead to unexpected behavior.\n\n6. **Testing**:\n   - Ensure that there are adequate unit tests covering this new method, especially for various combinations of input parameters, including edge cases like empty lists or null values.\n\nIn summary, while the code introduces a useful feature, addressing the above points will enhance its robustness and maintainability.",
          "created_at": "2025-04-29T01:07:52"
        },
        {
          "id": 28,
          "review_id": 2,
          "pr_id": 2487253010,
          "filename": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/PlannerUtils.java",
          "content": "Overall, the code diff introduces a new method `requireTimeSeriesSource` that checks if any part of a `PhysicalPlan` contains a time series source. The implementation appears to be straightforward, but there are a few areas that could be improved or warrant further consideration.\n\n1. **Code Style and Best Practices**:\n   - **Line 159**: The use of the `instanceof` operator with pattern matching (e.g., `if (p instanceof FragmentExec f)`) is a nice modern Java feature. However, ensure that this is consistent with the project's Java version requirements. If the project supports Java 16 or higher, this is fine; otherwise, consider using the traditional casting approach for compatibility.\n   - **Line 161**: The nested `instanceof` check could be simplified by using a guard clause. Instead of returning `false` in the `else` block, you could return `false` immediately if `p` is not an instance of `FragmentExec`.\n\n2. **Performance Issues**:\n   - The method uses `anyMatch`, which is efficient for short-circuiting the evaluation. However, if `plan` can be large, consider whether the structure of `PhysicalPlan` allows for early exits or optimizations. If `plan` is frequently large, profiling may be necessary to ensure performance is acceptable.\n\n3. **Potential Edge Cases**:\n   - **Line 158**: If `plan` is `null`, this will throw a `NullPointerException`. Consider adding a null check at the beginning of the method to handle this gracefully.\n   - The method assumes that `FragmentExec` and `EsRelation` are well-defined and that `indexMode()` will not return `null`. If there's a possibility of `null` values, you should add checks to avoid potential `NullPointerExceptions`.\n\n4. **Documentation**:\n   - There are no comments or documentation for the new method. Adding a brief JavaDoc comment explaining the purpose of `requireTimeSeriesSource`, its parameters, and return value would improve code readability and maintainability.\n\n5. **Unit Tests**:\n   - Ensure that there are corresponding unit tests for this new method. Testing various scenarios, including edge cases (e.g., empty plans, plans with no `FragmentExec`, etc.), will help ensure the method behaves as expected.\n\nIn summary, while the new method is a useful addition, it could benefit from additional null checks, documentation, and consideration of edge cases. Addressing these points will enhance the robustness and maintainability of the code.\n\n**Actionable Feedback**:\n- Add a null check for `plan` at the start of `requireTimeSeriesSource`.\n- Consider using a guard clause for the `else` statement.\n- Add JavaDoc comments for the new method.\n- Ensure unit tests cover various scenarios for the new method.\n\nOverall, the implementation is on the right track but could be improved with these considerations.",
          "created_at": "2025-04-29T01:07:52"
        },
        {
          "id": 29,
          "review_id": 2,
          "pr_id": 2487253010,
          "filename": "x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/planner/TestPhysicalOperationProviders.java",
          "content": "Overall, the code diff appears to be a well-structured addition to the `TestPhysicalOperationProviders` class, introducing a new method for handling time series aggregation. However, there are a few points worth noting for improvement and clarity.\n\n1. **Method Naming**:\n   - **Line 156**: The method name `timeSeriesAggregatorOperatorFactor` seems to be a typo. It should likely be `timeSeriesAggregatorOperatorFactory` to maintain consistency with the naming convention of factory methods. This could lead to confusion for anyone reading the code.\n\n2. **Parameter Validation**:\n   - **Line 157-162**: There is no validation for the parameters being passed into the `timeSeriesAggregatorOperatorFactory` method. It would be prudent to check if `ts`, `aggregatorMode`, `aggregatorFactories`, and `groupSpecs` are not null before proceeding. This can prevent potential `NullPointerExceptions` during runtime.\n\n3. **Documentation**:\n   - There are no comments or documentation for the new method. Adding JavaDoc comments to describe the purpose of the method, its parameters, and return value would enhance code readability and maintainability.\n\n4. **Performance Considerations**:\n   - **Line 162**: The method creates a new instance of `TimeSeriesAggregationOperator.Factory`. Depending on how often this method is called, consider whether this factory instantiation could be optimized or reused to avoid unnecessary object creation.\n\n5. **Edge Cases**:\n   - Consider what happens if `context.pageSize(ts.estimatedRowSize())` returns an unexpected value (e.g., zero or negative). It would be beneficial to handle such cases to avoid runtime errors.\n\n6. **Code Style**:\n   - Ensure consistent formatting and spacing throughout the code. For example, there is a mix of spaces and tabs in the surrounding code. Consistency in indentation style improves readability.\n\nIn summary, while the new method adds functionality, addressing the naming issue, adding parameter validation, improving documentation, and considering performance and edge cases will enhance the quality of the code.",
          "created_at": "2025-04-29T01:07:52"
        }
      ]
    }
  ]
}